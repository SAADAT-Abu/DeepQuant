{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b94d5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDeepQuant V2: Gene-Aware Transcriptome Quantification with Hierarchical Learning\\n================================================================================\\n\\nA next-generation transcriptome quantification tool that leverages gene family relationships\\nfor improved isoform disambiguation using transformer architectures and probabilistic modeling.\\n\\nKey Features:\\n- Gene-aware hierarchical modeling\\n- Multi-level contrastive learning  \\n- Uncertainty-aware assignment strategies\\n- Integration with classical statistical methods\\n- Comprehensive evaluation framework\\n\\nVersion: 2.0\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DeepQuant V2: Gene-Aware Transcriptome Quantification with Hierarchical Learning\n",
    "================================================================================\n",
    "\n",
    "A next-generation transcriptome quantification tool that leverages gene family relationships\n",
    "for improved isoform disambiguation using transformer architectures and probabilistic modeling.\n",
    "\n",
    "Key Features:\n",
    "- Gene-aware hierarchical modeling\n",
    "- Multi-level contrastive learning  \n",
    "- Uncertainty-aware assignment strategies\n",
    "- Integration with classical statistical methods\n",
    "- Comprehensive evaluation framework\n",
    "\n",
    "Version: 2.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2674f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import faiss\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from scipy.stats import entropy\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf34a1a9",
   "metadata": {},
   "source": [
    "### Configuration and Data Structures  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fca9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DeepQuantConfig:\n",
    "    \"\"\"Configuration class for DeepQuant V2.\"\"\"\n",
    "    \n",
    "    # Model architecture\n",
    "    model_name: str = \"zhihan1996/DNABERT-2-117M\"\n",
    "    embedding_dim: int = 256\n",
    "    gene_embedding_dim: int = 128\n",
    "    dropout: float = 0.1\n",
    "    num_attention_heads: int = 8\n",
    "    hidden_dim_multiplier: int = 4\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    num_epochs: int = 10\n",
    "    warmup_steps: int = 1000\n",
    "    \n",
    "    # Gene-aware learning\n",
    "    gene_contrastive_weight: float = 0.3\n",
    "    isoform_contrastive_weight: float = 0.7\n",
    "    gene_hierarchy_weight: float = 0.2\n",
    "    temperature: float = 0.1\n",
    "    \n",
    "    # Assignment strategy\n",
    "    assignment_strategy: str = \"hierarchical\"  # \"hierarchical\", \"joint\", \"ensemble\"\n",
    "    gene_similarity_threshold: float = 0.7\n",
    "    isoform_similarity_threshold: float = 0.85\n",
    "    uncertainty_threshold: float = 0.8\n",
    "    \n",
    "    # Computational\n",
    "    max_sequence_length: int = 512\n",
    "    use_mixed_precision: bool = True\n",
    "    num_workers: int = 4\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Output and logging\n",
    "    output_dir: str = \"./deepquant_v2_results\"\n",
    "    log_level: str = \"INFO\"\n",
    "    save_attention_maps: bool = False\n",
    "\n",
    "@dataclass \n",
    "class TranscriptInfo:\n",
    "    \"\"\"Information about a single transcript.\"\"\"\n",
    "    transcript_id: str\n",
    "    gene_id: str\n",
    "    gene_name: str\n",
    "    sequence: str\n",
    "    length: int\n",
    "    transcript_type: str\n",
    "    \n",
    "@dataclass\n",
    "class GeneFamilyInfo:\n",
    "    \"\"\"Information about a gene family and its isoforms.\"\"\"\n",
    "    gene_name: str\n",
    "    gene_id: str\n",
    "    transcripts: Dict[str, TranscriptInfo]\n",
    "    similarity_matrix: Optional[np.ndarray] = None\n",
    "    complexity_score: float = 0.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.num_isoforms = len(self.transcripts)\n",
    "        self.transcript_ids = list(self.transcripts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f68d1",
   "metadata": {},
   "source": [
    "### Data Loading and Gene Family Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9de192e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptomeParser:\n",
    "    \"\"\"Parse transcriptome FASTA and organize by gene families.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def parse_fasta_header(self, header: str) -> Dict[str, str]:\n",
    "        \"\"\"Parse GENCODE-style FASTA header - extract only essential fields.\"\"\"\n",
    "        # Remove '>' and split by '|'\n",
    "        fields = header.split('|')\n",
    "        \n",
    "        if len(fields) < 6:  # Need at least transcript_id and gene_name\n",
    "            self.logger.warning(f\"Header missing essential fields: {header}\")\n",
    "            return None\n",
    "            \n",
    "        return {\n",
    "            'transcript_id': fields[0],          # First field: transcript ID\n",
    "            'gene_name': fields[5],              # Sixth field: gene name  \n",
    "            'gene_id': fields[1] if len(fields) > 1 else fields[0],  # Fallback to transcript_id\n",
    "            'transcript_type': fields[7] if len(fields) > 7 else 'unknown'  # Optional\n",
    "        }\n",
    "    \n",
    "    def load_transcriptome(self, fasta_path: str) -> Tuple[Dict[str, GeneFamilyInfo], Dict[str, TranscriptInfo]]:\n",
    "        \"\"\"Load transcriptome and organize by gene families.\"\"\"\n",
    "        self.logger.info(f\"Loading transcriptome from {fasta_path}\")\n",
    "        \n",
    "        transcripts = {}\n",
    "        gene_families = defaultdict(lambda: {\n",
    "            'gene_name': '',\n",
    "            'gene_id': '',\n",
    "            'transcripts': {}\n",
    "        })\n",
    "        \n",
    "        skipped = 0\n",
    "        loaded = 0\n",
    "        \n",
    "        try:\n",
    "            for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "                header_info = self.parse_fasta_header(record.id)\n",
    "                if not header_info:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Skip very short sequences\n",
    "                if len(record.seq) < 50:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                transcript_info = TranscriptInfo(\n",
    "                    transcript_id=header_info['transcript_id'],\n",
    "                    gene_id=header_info['gene_id'],\n",
    "                    gene_name=header_info['gene_name'],\n",
    "                    sequence=str(record.seq).upper(),\n",
    "                    length=len(record.seq),\n",
    "                    transcript_type=header_info['transcript_type']\n",
    "                )\n",
    "                \n",
    "                transcripts[transcript_info.transcript_id] = transcript_info\n",
    "                \n",
    "                # Group by gene name\n",
    "                gene_name = header_info['gene_name']\n",
    "                gene_families[gene_name]['gene_name'] = gene_name\n",
    "                gene_families[gene_name]['gene_id'] = header_info['gene_id']\n",
    "                gene_families[gene_name]['transcripts'][transcript_info.transcript_id] = transcript_info\n",
    "                \n",
    "                loaded += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading transcriptome: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Convert to GeneFamilyInfo objects\n",
    "        final_gene_families = {}\n",
    "        for gene_name, info in gene_families.items():\n",
    "            final_gene_families[gene_name] = GeneFamilyInfo(\n",
    "                gene_name=info['gene_name'],\n",
    "                gene_id=info['gene_id'],\n",
    "                transcripts=info['transcripts']\n",
    "            )\n",
    "        \n",
    "        self.logger.info(f\"Loaded {loaded} transcripts, {len(final_gene_families)} gene families\")\n",
    "        self.logger.info(f\"Skipped {skipped} transcripts\")\n",
    "        \n",
    "        return final_gene_families, transcripts\n",
    "    \n",
    "    def analyze_gene_families(self, gene_families: Dict[str, GeneFamilyInfo]) -> Dict[str, any]:\n",
    "        \"\"\"Analyze gene family complexity and similarity patterns.\"\"\"\n",
    "        self.logger.info(\"Analyzing gene family complexity...\")\n",
    "        \n",
    "        stats = {\n",
    "            'total_genes': len(gene_families),\n",
    "            'total_transcripts': sum(len(gf.transcripts) for gf in gene_families.values()),\n",
    "            'single_isoform_genes': 0,\n",
    "            'multi_isoform_genes': 0,\n",
    "            'max_isoforms': 0,\n",
    "            'complex_genes': [],\n",
    "            'isoform_distribution': Counter()\n",
    "        }\n",
    "        \n",
    "        for gene_name, gene_family in gene_families.items():\n",
    "            num_isoforms = len(gene_family.transcripts)\n",
    "            stats['isoform_distribution'][num_isoforms] += 1\n",
    "            \n",
    "            if num_isoforms == 1:\n",
    "                stats['single_isoform_genes'] += 1\n",
    "            else:\n",
    "                stats['multi_isoform_genes'] += 1\n",
    "                \n",
    "            if num_isoforms > stats['max_isoforms']:\n",
    "                stats['max_isoforms'] = num_isoforms\n",
    "            \n",
    "            # Mark complex genes (>5 isoforms) for special attention\n",
    "            if num_isoforms > 5:\n",
    "                stats['complex_genes'].append((gene_name, num_isoforms))\n",
    "        \n",
    "        self.logger.info(f\"Gene family analysis complete:\")\n",
    "        self.logger.info(f\"  Total genes: {stats['total_genes']}\")\n",
    "        self.logger.info(f\"  Single isoform genes: {stats['single_isoform_genes']}\")\n",
    "        self.logger.info(f\"  Multi-isoform genes: {stats['multi_isoform_genes']}\")\n",
    "        self.logger.info(f\"  Most complex gene: {stats['max_isoforms']} isoforms\")\n",
    "        \n",
    "        return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc58fa5",
   "metadata": {},
   "source": [
    "### Gene-Aware Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f089387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneAwareDataset(Dataset):\n",
    "    \"\"\"Dataset that provides gene family context for training.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 reads: List[str],\n",
    "                 ground_truth: pd.DataFrame,\n",
    "                 gene_families: Dict[str, GeneFamilyInfo],\n",
    "                 transcripts: Dict[str, TranscriptInfo],\n",
    "                 config: DeepQuantConfig,\n",
    "                 mode: str = \"train\"):\n",
    "        \n",
    "        self.reads = reads\n",
    "        self.ground_truth = ground_truth\n",
    "        self.gene_families = gene_families\n",
    "        self.transcripts = transcripts\n",
    "        self.config = config\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Create mappings\n",
    "        self.transcript_to_gene = {t_id: info.gene_name \n",
    "                                  for t_id, info in transcripts.items()}\n",
    "        \n",
    "        # Prepare tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        \n",
    "        # Create gene family lookup for efficient sampling\n",
    "        self.gene_to_transcripts = defaultdict(list)\n",
    "        for t_id, info in transcripts.items():\n",
    "            self.gene_to_transcripts[info.gene_name].append(t_id)\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reads)\n",
    "    \n",
    "    def tokenize_sequence(self, sequence: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Tokenize DNA sequence with proper truncation/padding.\"\"\"\n",
    "        sequence = sequence.upper().replace('N', 'A')\n",
    "        \n",
    "        tokens = self.tokenizer(\n",
    "            sequence,\n",
    "            max_length=self.config.max_sequence_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        return {k: v.squeeze(0) for k, v in tokens.items()}\n",
    "    \n",
    "    def get_gene_family_context(self, transcript_id: str) -> Dict[str, any]:\n",
    "        \"\"\"Get gene family context for a transcript.\"\"\"\n",
    "        transcript_info = self.transcripts[transcript_id]\n",
    "        gene_name = transcript_info.gene_name\n",
    "        gene_family = self.gene_families[gene_name]\n",
    "        \n",
    "        # Get all isoforms in the same gene family\n",
    "        family_transcripts = list(gene_family.transcripts.keys())\n",
    "        \n",
    "        # Sample negative transcripts from different genes\n",
    "        negative_genes = [g for g in self.gene_families.keys() if g != gene_name]\n",
    "        negative_transcripts = []\n",
    "        if negative_genes:\n",
    "            neg_gene = np.random.choice(negative_genes)\n",
    "            neg_family = self.gene_families[neg_gene]\n",
    "            negative_transcripts = list(neg_family.transcripts.keys())\n",
    "        \n",
    "        return {\n",
    "            'gene_name': gene_name,\n",
    "            'family_transcripts': family_transcripts,\n",
    "            'negative_transcripts': negative_transcripts,\n",
    "            'num_isoforms': len(family_transcripts),\n",
    "            'is_complex_gene': len(family_transcripts) > 3\n",
    "        }\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        read_sequence = self.reads[idx]\n",
    "        \n",
    "        # Get ground truth information\n",
    "        if self.ground_truth is not None and idx < len(self.ground_truth):\n",
    "            gt_row = self.ground_truth.iloc[idx]\n",
    "            true_transcript = gt_row['true_transcript']\n",
    "        else:\n",
    "            # For inference mode\n",
    "            true_transcript = None\n",
    "        \n",
    "        # Tokenize the read\n",
    "        read_tokens = self.tokenize_sequence(read_sequence)\n",
    "        \n",
    "        sample = {\n",
    "            'read_id': f\"read_{idx:06d}\",\n",
    "            'read_sequence': read_sequence,\n",
    "            'input_ids': read_tokens['input_ids'],\n",
    "            'attention_mask': read_tokens['attention_mask'],\n",
    "            'read_idx': idx\n",
    "        }\n",
    "        \n",
    "        # Add training-specific information\n",
    "        if self.mode == \"train\" and true_transcript is not None:\n",
    "            sample['true_transcript'] = true_transcript\n",
    "            \n",
    "            # Get gene family context\n",
    "            if true_transcript in self.transcripts:\n",
    "                try:\n",
    "                    gene_context = self.get_gene_family_context(true_transcript)\n",
    "                    sample.update({\n",
    "                        'gene_name': gene_context['gene_name'],\n",
    "                        'family_transcripts': gene_context['family_transcripts'],\n",
    "                        'negative_transcripts': gene_context['negative_transcripts'],\n",
    "                        'num_isoforms': gene_context['num_isoforms'],\n",
    "                        'is_complex_gene': gene_context['is_complex_gene']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error getting gene context for {true_transcript}: {e}\")\n",
    "                    # Provide fallback values\n",
    "                    sample.update({\n",
    "                        'gene_name': None,\n",
    "                        'family_transcripts': [],\n",
    "                        'negative_transcripts': [],\n",
    "                        'num_isoforms': 1,\n",
    "                        'is_complex_gene': False\n",
    "                    })\n",
    "            else:\n",
    "                self.logger.warning(f\"Unknown transcript in ground truth: {true_transcript}\")\n",
    "                # Provide fallback values\n",
    "                sample.update({\n",
    "                    'gene_name': None,\n",
    "                    'family_transcripts': [],\n",
    "                    'negative_transcripts': [],\n",
    "                    'num_isoforms': 1,\n",
    "                    'is_complex_gene': False\n",
    "                })\n",
    "        \n",
    "        return sample\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom collate function that handles variable-length gene family information.\"\"\"\n",
    "    \n",
    "    # Tensor fields that are already tensors\n",
    "    tensor_fields = ['input_ids', 'attention_mask']\n",
    "    collated = {}\n",
    "    \n",
    "    for field in tensor_fields:\n",
    "        if field in batch[0]:\n",
    "            collated[field] = torch.stack([sample[field] for sample in batch])\n",
    "    \n",
    "    # Handle read_idx separately (it's an integer, needs to be converted to tensor)\n",
    "    if 'read_idx' in batch[0]:\n",
    "        collated['read_idx'] = torch.tensor([sample['read_idx'] for sample in batch])\n",
    "    \n",
    "    # List fields\n",
    "    list_fields = ['read_id', 'read_sequence', 'true_transcript', 'gene_name']\n",
    "    for field in list_fields:\n",
    "        if field in batch[0]:\n",
    "            collated[field] = [sample[field] for sample in batch]\n",
    "    \n",
    "    # Gene family context (variable length)\n",
    "    if 'family_transcripts' in batch[0]:\n",
    "        collated['family_transcripts'] = [sample['family_transcripts'] for sample in batch]\n",
    "        collated['negative_transcripts'] = [sample['negative_transcripts'] for sample in batch]\n",
    "        collated['num_isoforms'] = torch.tensor([sample['num_isoforms'] for sample in batch])\n",
    "        collated['is_complex_gene'] = torch.tensor([sample['is_complex_gene'] for sample in batch])\n",
    "    \n",
    "    return collated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795fe16",
   "metadata": {},
   "source": [
    "### Neural Architecture Components  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd539119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleAttention(nn.Module):\n",
    "    \"\"\"Multi-scale attention mechanism for capturing both local and global patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Multi-scale attention heads\n",
    "        self.local_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads // 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.global_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads // 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.scale_fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        \n",
    "        # Convert attention mask to key_padding_mask format if provided\n",
    "        key_padding_mask = None\n",
    "        if attention_mask is not None:\n",
    "            # attention_mask: 1 for real tokens, 0 for padding\n",
    "            # key_padding_mask: True for padding tokens, False for real tokens\n",
    "            key_padding_mask = (attention_mask == 0)\n",
    "        \n",
    "        # Local attention with restricted window\n",
    "        local_out, local_weights = self.local_attention(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        # Global attention \n",
    "        global_out, global_weights = self.global_attention(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        # Fuse multi-scale features\n",
    "        fused = torch.cat([local_out, global_out], dim=-1)\n",
    "        output = self.scale_fusion(fused)\n",
    "        \n",
    "        return output, {'local_weights': local_weights, 'global_weights': global_weights}\n",
    "\n",
    "class GeneHierarchicalEncoder(nn.Module):\n",
    "    \"\"\"Hierarchical encoder that learns gene-level and isoform-level representations.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Base transformer encoder\n",
    "        self.base_encoder = AutoModel.from_pretrained(config.model_name)\n",
    "        base_hidden_size = self.base_encoder.config.hidden_size\n",
    "        \n",
    "        # Gene-level encoder\n",
    "        self.gene_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=base_hidden_size,\n",
    "                nhead=config.num_attention_heads,\n",
    "                dim_feedforward=base_hidden_size * config.hidden_dim_multiplier,\n",
    "                dropout=config.dropout,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Isoform-specific encoder\n",
    "        self.isoform_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=base_hidden_size,\n",
    "                nhead=config.num_attention_heads,\n",
    "                dim_feedforward=base_hidden_size * config.hidden_dim_multiplier,\n",
    "                dropout=config.dropout,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Multi-scale attention\n",
    "        self.multi_scale_attention = MultiScaleAttention(\n",
    "            hidden_dim=base_hidden_size,\n",
    "            num_heads=config.num_attention_heads,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        \n",
    "        # Projection heads\n",
    "        self.gene_projection = nn.Sequential(\n",
    "            nn.Linear(base_hidden_size, config.gene_embedding_dim * 2),\n",
    "            nn.LayerNorm(config.gene_embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.gene_embedding_dim * 2, config.gene_embedding_dim),\n",
    "            nn.LayerNorm(config.gene_embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.isoform_projection = nn.Sequential(\n",
    "            nn.Linear(base_hidden_size, config.embedding_dim * 2),\n",
    "            nn.LayerNorm(config.embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.embedding_dim * 2, config.embedding_dim),\n",
    "            nn.LayerNorm(config.embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Uncertainty estimation heads\n",
    "        self.gene_uncertainty = nn.Sequential(\n",
    "            nn.Linear(config.gene_embedding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.isoform_uncertainty = nn.Sequential(\n",
    "            nn.Linear(config.embedding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Base encoding\n",
    "        base_output = self.base_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = base_output.last_hidden_state\n",
    "        \n",
    "        # Multi-scale attention\n",
    "        enhanced_output, attention_weights = self.multi_scale_attention(\n",
    "            sequence_output, attention_mask\n",
    "        )\n",
    "        \n",
    "        # Convert attention mask to proper format for transformer layers\n",
    "        # attention_mask: 1 for real tokens, 0 for padding\n",
    "        # key_padding_mask: True for padding tokens, False for real tokens\n",
    "        padding_mask = (attention_mask == 0)  # Convert to boolean mask\n",
    "\n",
    "        # Gene-level encoding (captures common patterns)\n",
    "        gene_features = self.gene_encoder(enhanced_output, src_key_padding_mask=padding_mask)\n",
    "\n",
    "        # Isoform-level encoding (captures discriminative patterns) \n",
    "        isoform_features = self.isoform_encoder(enhanced_output, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Global pooling with attention mask\n",
    "        gene_pooled = self._attention_pooling(gene_features, attention_mask)\n",
    "        isoform_pooled = self._attention_pooling(isoform_features, attention_mask)\n",
    "        \n",
    "        # Project to embedding spaces\n",
    "        gene_embedding = self.gene_projection(gene_pooled)\n",
    "        isoform_embedding = self.isoform_projection(isoform_pooled)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        gene_embedding = F.normalize(gene_embedding, p=2, dim=-1)\n",
    "        isoform_embedding = F.normalize(isoform_embedding, p=2, dim=-1)\n",
    "        \n",
    "        # Uncertainty estimates\n",
    "        gene_uncertainty = self.gene_uncertainty(gene_embedding)\n",
    "        isoform_uncertainty = self.isoform_uncertainty(isoform_embedding)\n",
    "        \n",
    "        return {\n",
    "            'gene_embedding': gene_embedding,\n",
    "            'isoform_embedding': isoform_embedding,\n",
    "            'gene_uncertainty': gene_uncertainty,\n",
    "            'isoform_uncertainty': isoform_uncertainty,\n",
    "            'attention_weights': attention_weights\n",
    "        }\n",
    "    \n",
    "    def _attention_pooling(self, features, attention_mask):\n",
    "        \"\"\"Attention-based pooling that respects padding.\"\"\"\n",
    "        # features: [batch_size, seq_len, hidden_dim]\n",
    "        # attention_mask: [batch_size, seq_len]\n",
    "        \n",
    "        masked_features = features * attention_mask.unsqueeze(-1)\n",
    "        pooled = masked_features.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddd149",
   "metadata": {},
   "source": [
    "### Gene-Aware Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c1b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneAwareContrastiveLoss(nn.Module):\n",
    "    \"\"\"Multi-level contrastive loss with gene family awareness.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.temperature = config.temperature\n",
    "        self.gene_weight = config.gene_contrastive_weight\n",
    "        self.isoform_weight = config.isoform_contrastive_weight\n",
    "        \n",
    "    def info_nce_loss(self, embeddings, labels, temperature):\n",
    "        \"\"\"Compute InfoNCE loss for contrastive learning.\"\"\"\n",
    "        batch_size = embeddings.shape[0]\n",
    "        \n",
    "        if batch_size < 2:\n",
    "            return torch.tensor(0.0, device=embeddings.device, requires_grad=True)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        sim_matrix = torch.matmul(embeddings, embeddings.T) / temperature\n",
    "        \n",
    "        # Create positive mask\n",
    "        labels = labels.view(-1, 1)\n",
    "        positive_mask = (labels == labels.T).float()\n",
    "        positive_mask.fill_diagonal_(0)  # Remove self-similarity\n",
    "        \n",
    "        # Check if there are any positive pairs\n",
    "        if positive_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=embeddings.device, requires_grad=True)\n",
    "        \n",
    "        # Numerically stable computation\n",
    "        # Subtract max for numerical stability\n",
    "        sim_matrix = sim_matrix - sim_matrix.max(dim=1, keepdim=True)[0].detach()\n",
    "        \n",
    "        exp_sim = torch.exp(sim_matrix)\n",
    "        \n",
    "        # Mask out diagonal (self-similarity)\n",
    "        mask = torch.eye(batch_size, device=embeddings.device, dtype=torch.bool)\n",
    "        exp_sim = exp_sim.masked_fill(mask, 0)\n",
    "        \n",
    "        # Compute positive and negative terms\n",
    "        pos_sim = (exp_sim * positive_mask).sum(dim=1)\n",
    "        neg_sim = exp_sim.sum(dim=1)\n",
    "        \n",
    "        # Avoid log(0) by adding small epsilon\n",
    "        eps = 1e-8\n",
    "        loss = -torch.log((pos_sim + eps) / (neg_sim + eps))\n",
    "        \n",
    "        # Only compute loss for samples that have positive pairs\n",
    "        valid_samples = positive_mask.sum(dim=1) > 0\n",
    "        if valid_samples.sum() == 0:\n",
    "            return torch.tensor(0.0, device=embeddings.device, requires_grad=True)\n",
    "        \n",
    "        return loss[valid_samples].mean()\n",
    "    \n",
    "    def forward(self, gene_embeddings, isoform_embeddings, gene_labels, transcript_labels):\n",
    "        \"\"\"\n",
    "        Compute multi-level contrastive loss.\n",
    "        \n",
    "        Args:\n",
    "            gene_embeddings: [batch_size, gene_embedding_dim]\n",
    "            isoform_embeddings: [batch_size, embedding_dim]  \n",
    "            gene_labels: [batch_size] - gene identifiers\n",
    "            transcript_labels: [batch_size] - transcript identifiers\n",
    "        \"\"\"\n",
    "        \n",
    "        # Gene-level contrastive loss (easier task)\n",
    "        gene_loss = self.info_nce_loss(gene_embeddings, gene_labels, self.temperature)\n",
    "        \n",
    "        # Isoform-level contrastive loss (harder task)\n",
    "        isoform_loss = self.info_nce_loss(isoform_embeddings, transcript_labels, self.temperature * 0.5)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = self.gene_weight * gene_loss + self.isoform_weight * isoform_loss\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'gene_loss': gene_loss,\n",
    "            'isoform_loss': isoform_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717ed70",
   "metadata": {},
   "source": [
    "### Vector Store and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "386e872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneAwareVectorStore:\n",
    "    \"\"\"Vector store with gene family awareness for efficient search.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        self.config = config\n",
    "        self.gene_families = None\n",
    "        self.transcripts = None\n",
    "        \n",
    "        # Separate indices for gene and isoform embeddings\n",
    "        self.gene_index = None\n",
    "        self.isoform_index = None\n",
    "        \n",
    "        # Mappings\n",
    "        self.transcript_to_idx = {}\n",
    "        self.idx_to_transcript = {}\n",
    "        self.transcript_to_gene = {}\n",
    "        \n",
    "        # Embeddings storage\n",
    "        self.gene_embeddings = None\n",
    "        self.isoform_embeddings = None\n",
    "        self.gene_uncertainties = None\n",
    "        self.isoform_uncertainties = None\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def build_index(self, \n",
    "                   gene_families: Dict[str, GeneFamilyInfo],\n",
    "                   transcripts: Dict[str, TranscriptInfo],\n",
    "                   model: nn.Module,\n",
    "                   tokenizer):\n",
    "        \"\"\"Build gene-aware search indices.\"\"\"\n",
    "        \n",
    "        self.gene_families = gene_families\n",
    "        self.transcripts = transcripts\n",
    "        \n",
    "        self.logger.info(\"Building gene-aware vector indices...\")\n",
    "        \n",
    "        # Prepare transcript data\n",
    "        transcript_ids = list(transcripts.keys())\n",
    "        transcript_sequences = [transcripts[tid].sequence for tid in transcript_ids]\n",
    "        \n",
    "        # Create mappings\n",
    "        for idx, tid in enumerate(transcript_ids):\n",
    "            self.transcript_to_idx[tid] = idx\n",
    "            self.idx_to_transcript[idx] = tid\n",
    "            self.transcript_to_gene[tid] = transcripts[tid].gene_name\n",
    "        \n",
    "        # Generate embeddings in batches\n",
    "        model.eval()\n",
    "        gene_embeddings_list = []\n",
    "        isoform_embeddings_list = []\n",
    "        gene_uncertainties_list = []\n",
    "        isoform_uncertainties_list = []\n",
    "        \n",
    "        batch_size = self.config.batch_size\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(transcript_sequences), batch_size):\n",
    "                batch_seqs = transcript_sequences[i:i+batch_size]\n",
    "                \n",
    "                # Tokenize batch\n",
    "                batch_tokens = []\n",
    "                for seq in batch_seqs:\n",
    "                    tokens = tokenizer(\n",
    "                        seq.upper().replace('N', 'A'),\n",
    "                        max_length=self.config.max_sequence_length,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    batch_tokens.append(tokens)\n",
    "                \n",
    "                # Stack tensors\n",
    "                input_ids = torch.stack([t['input_ids'].squeeze(0) for t in batch_tokens]).to(self.config.device)\n",
    "                attention_mask = torch.stack([t['attention_mask'].squeeze(0) for t in batch_tokens]).to(self.config.device)\n",
    "                \n",
    "                # Get embeddings\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                \n",
    "                gene_embeddings_list.append(outputs['gene_embedding'].cpu().numpy())\n",
    "                isoform_embeddings_list.append(outputs['isoform_embedding'].cpu().numpy())\n",
    "                gene_uncertainties_list.append(outputs['gene_uncertainty'].cpu().numpy())\n",
    "                isoform_uncertainties_list.append(outputs['isoform_uncertainty'].cpu().numpy())\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        self.gene_embeddings = np.vstack(gene_embeddings_list).astype('float32')\n",
    "        self.isoform_embeddings = np.vstack(isoform_embeddings_list).astype('float32')\n",
    "        self.gene_uncertainties = np.vstack(gene_uncertainties_list).astype('float32')\n",
    "        self.isoform_uncertainties = np.vstack(isoform_uncertainties_list).astype('float32')\n",
    "        \n",
    "        # Build FAISS indices\n",
    "        gene_dim = self.gene_embeddings.shape[1]\n",
    "        isoform_dim = self.isoform_embeddings.shape[1]\n",
    "        \n",
    "        self.gene_index = faiss.IndexFlatIP(gene_dim)\n",
    "        self.isoform_index = faiss.IndexFlatIP(isoform_dim)\n",
    "        \n",
    "        self.gene_index.add(self.gene_embeddings)\n",
    "        self.isoform_index.add(self.isoform_embeddings)\n",
    "        \n",
    "        self.logger.info(f\"Built indices with {len(transcript_ids)} transcripts\")\n",
    "        self.logger.info(f\"Gene embedding dim: {gene_dim}, Isoform embedding dim: {isoform_dim}\")\n",
    "    \n",
    "    def hierarchical_search(self, \n",
    "                          gene_embedding: np.ndarray,\n",
    "                          isoform_embedding: np.ndarray,\n",
    "                          top_k_genes: int = 10,\n",
    "                          top_k_isoforms: int = 5) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Hierarchical search: first find candidate genes, then best isoforms within those genes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Gene-level search\n",
    "        gene_similarities, gene_indices = self.gene_index.search(\n",
    "            gene_embedding.astype('float32'), top_k_genes\n",
    "        )\n",
    "        \n",
    "        # Get candidate genes\n",
    "        candidate_transcripts = []\n",
    "        candidate_genes = set()\n",
    "        \n",
    "        for idx in gene_indices[0]:\n",
    "            transcript_id = self.idx_to_transcript[idx]\n",
    "            gene_name = self.transcript_to_gene[transcript_id]\n",
    "            candidate_genes.add(gene_name)\n",
    "        \n",
    "        # Step 2: Collect all transcripts from candidate genes\n",
    "        for gene_name in candidate_genes:\n",
    "            gene_family = self.gene_families[gene_name]\n",
    "            candidate_transcripts.extend(gene_family.transcript_ids)\n",
    "        \n",
    "        # Step 3: Isoform-level search within candidates\n",
    "        if candidate_transcripts:\n",
    "            candidate_indices = [self.transcript_to_idx[tid] for tid in candidate_transcripts]\n",
    "            candidate_embeddings = self.isoform_embeddings[candidate_indices]\n",
    "            \n",
    "            # Compute similarities\n",
    "            isoform_similarities = np.dot(candidate_embeddings, isoform_embedding.T).flatten()\n",
    "            \n",
    "            # Get top isoforms\n",
    "            top_indices = np.argsort(isoform_similarities)[-top_k_isoforms:][::-1]\n",
    "            \n",
    "            results = []\n",
    "            for i, idx in enumerate(top_indices):\n",
    "                global_idx = candidate_indices[idx]\n",
    "                transcript_id = self.idx_to_transcript[global_idx]\n",
    "                gene_name = self.transcript_to_gene[transcript_id]\n",
    "                \n",
    "                results.append({\n",
    "                    'transcript_id': transcript_id,\n",
    "                    'gene_name': gene_name,\n",
    "                    'gene_similarity': float(gene_similarities[0][0]),  # Use first gene match\n",
    "                    'isoform_similarity': float(isoform_similarities[idx]),\n",
    "                    'gene_uncertainty': float(self.gene_uncertainties[global_idx][0]),\n",
    "                    'isoform_uncertainty': float(self.isoform_uncertainties[global_idx][0]),\n",
    "                    'global_idx': global_idx\n",
    "                })\n",
    "        else:\n",
    "            results = []\n",
    "        \n",
    "        return {\n",
    "            'results': results,\n",
    "            'candidate_genes': list(candidate_genes),\n",
    "            'num_candidates': len(candidate_transcripts)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71758c",
   "metadata": {},
   "source": [
    "### Uncertainty-Aware Assignment Engine  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b241647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncertaintyAwareAssignment:\n",
    "    \"\"\"Advanced assignment engine with multiple strategies and uncertainty quantification.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Assignment thresholds (will be learned/adapted)\n",
    "        self.gene_threshold = config.gene_similarity_threshold\n",
    "        self.isoform_threshold = config.isoform_similarity_threshold\n",
    "        self.uncertainty_threshold = config.uncertainty_threshold\n",
    "        \n",
    "    def compute_assignment_confidence(self, \n",
    "                                    gene_similarity: float,\n",
    "                                    isoform_similarity: float,\n",
    "                                    gene_uncertainty: float,\n",
    "                                    isoform_uncertainty: float,\n",
    "                                    num_candidates: int) -> float:\n",
    "        \"\"\"Compute overall assignment confidence score.\"\"\"\n",
    "        \n",
    "        # Similarity component (higher = better)\n",
    "        sim_score = 0.4 * gene_similarity + 0.6 * isoform_similarity\n",
    "        \n",
    "        # Uncertainty component (lower = better, so we take 1 - uncertainty)\n",
    "        uncertainty_score = 0.4 * (1 - gene_uncertainty) + 0.6 * (1 - isoform_uncertainty)\n",
    "        \n",
    "        # Complexity penalty (more candidates = lower confidence)\n",
    "        complexity_penalty = 1.0 / np.log(num_candidates + 2)\n",
    "        \n",
    "        # Combined confidence\n",
    "        confidence = sim_score * uncertainty_score * complexity_penalty\n",
    "        \n",
    "        return np.clip(confidence, 0.0, 1.0)\n",
    "    \n",
    "    def adaptive_threshold_selection(self,\n",
    "                                   similarities: List[float],\n",
    "                                   uncertainties: List[float],\n",
    "                                   gene_complexity: int) -> float:\n",
    "        \"\"\"Adaptively select similarity threshold based on context.\"\"\"\n",
    "        \n",
    "        if not similarities:\n",
    "            return self.isoform_threshold\n",
    "        \n",
    "        # Base threshold\n",
    "        base_threshold = self.isoform_threshold\n",
    "        \n",
    "        # Adjust based on gene complexity\n",
    "        complexity_adjustment = min(0.1, gene_complexity * 0.02)\n",
    "        \n",
    "        # Adjust based on uncertainty distribution\n",
    "        mean_uncertainty = np.mean(uncertainties)\n",
    "        uncertainty_adjustment = mean_uncertainty * 0.1\n",
    "        \n",
    "        # Adjust based on similarity gap\n",
    "        if len(similarities) > 1:\n",
    "            top_sim = max(similarities)\n",
    "            second_sim = sorted(similarities, reverse=True)[1]\n",
    "            gap = top_sim - second_sim\n",
    "            gap_adjustment = -gap * 0.05  # Lower threshold if there's a clear winner\n",
    "        else:\n",
    "            gap_adjustment = 0.0\n",
    "        \n",
    "        adaptive_threshold = base_threshold + complexity_adjustment + uncertainty_adjustment + gap_adjustment\n",
    "        \n",
    "        return np.clip(adaptive_threshold, 0.3, 0.95)\n",
    "    \n",
    "    def hierarchical_assignment(self, search_results: Dict[str, any]) -> Dict[str, any]:\n",
    "        \"\"\"Hierarchical assignment strategy: gene first, then isoform.\"\"\"\n",
    "        \n",
    "        if not search_results['results']:\n",
    "            return {\n",
    "                'assigned_transcript': None,\n",
    "                'assigned_gene': None,\n",
    "                'confidence': 0.0,\n",
    "                'assignment_type': 'unassigned',\n",
    "                'reason': 'no_candidates'\n",
    "            }\n",
    "        \n",
    "        results = search_results['results']\n",
    "        \n",
    "        # Step 1: Gene-level filtering\n",
    "        gene_candidates = [r for r in results if r['gene_similarity'] >= self.gene_threshold]\n",
    "        \n",
    "        if not gene_candidates:\n",
    "            return {\n",
    "                'assigned_transcript': None,\n",
    "                'assigned_gene': None,\n",
    "                'confidence': 0.0,\n",
    "                'assignment_type': 'unassigned',\n",
    "                'reason': 'gene_threshold_not_met'\n",
    "            }\n",
    "        \n",
    "        # Step 2: Adaptive isoform threshold\n",
    "        isoform_sims = [r['isoform_similarity'] for r in gene_candidates]\n",
    "        isoform_uncs = [r['isoform_uncertainty'] for r in gene_candidates]\n",
    "        \n",
    "        adaptive_threshold = self.adaptive_threshold_selection(\n",
    "            isoform_sims, isoform_uncs, len(search_results['candidate_genes'])\n",
    "        )\n",
    "        \n",
    "        # Step 3: Isoform-level filtering\n",
    "        isoform_candidates = [r for r in gene_candidates \n",
    "                            if r['isoform_similarity'] >= adaptive_threshold]\n",
    "        \n",
    "        if not isoform_candidates:\n",
    "            return {\n",
    "                'assigned_transcript': None,\n",
    "                'assigned_gene': gene_candidates[0]['gene_name'],  # At least assign gene\n",
    "                'confidence': 0.0,\n",
    "                'assignment_type': 'gene_only',\n",
    "                'reason': 'isoform_threshold_not_met'\n",
    "            }\n",
    "        \n",
    "        # Step 4: Select best candidate\n",
    "        best_candidate = max(isoform_candidates, \n",
    "                           key=lambda x: self.compute_assignment_confidence(\n",
    "                               x['gene_similarity'], x['isoform_similarity'],\n",
    "                               x['gene_uncertainty'], x['isoform_uncertainty'],\n",
    "                               len(isoform_candidates)\n",
    "                           ))\n",
    "        \n",
    "        confidence = self.compute_assignment_confidence(\n",
    "            best_candidate['gene_similarity'], best_candidate['isoform_similarity'],\n",
    "            best_candidate['gene_uncertainty'], best_candidate['isoform_uncertainty'],\n",
    "            len(isoform_candidates)\n",
    "        )\n",
    "        \n",
    "        # Determine assignment type\n",
    "        if confidence >= self.uncertainty_threshold:\n",
    "            assignment_type = 'high_confidence'\n",
    "        elif len(isoform_candidates) == 1:\n",
    "            assignment_type = 'unique_match'\n",
    "        else:\n",
    "            assignment_type = 'best_match'\n",
    "        \n",
    "        return {\n",
    "            'assigned_transcript': best_candidate['transcript_id'],\n",
    "            'assigned_gene': best_candidate['gene_name'],\n",
    "            'confidence': confidence,\n",
    "            'assignment_type': assignment_type,\n",
    "            'reason': 'successful_assignment',\n",
    "            'num_gene_candidates': len(gene_candidates),\n",
    "            'num_isoform_candidates': len(isoform_candidates),\n",
    "            'adaptive_threshold': adaptive_threshold,\n",
    "            'all_candidates': isoform_candidates[:3]  # Keep top 3 for analysis\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4cdbe",
   "metadata": {},
   "source": [
    "### DeepQuantV2 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8562f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQuantV2:\n",
    "    \"\"\"\n",
    "    Main DeepQuant V2 class that orchestrates the entire pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.device)\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=getattr(logging, config.log_level),\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.parser = TranscriptomeParser(config)\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.vector_store = GeneAwareVectorStore(config)\n",
    "        self.assignment_engine = UncertaintyAwareAssignment(config)\n",
    "        \n",
    "        # Data storage\n",
    "        self.gene_families = None\n",
    "        self.transcripts = None\n",
    "        self.training_stats = {}\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    def load_data(self, transcriptome_path: str, reads_path: str = None, ground_truth_path: str = None):\n",
    "        \"\"\"Load transcriptome, reads, and ground truth data.\"\"\"\n",
    "        \n",
    "        # Load transcriptome\n",
    "        self.gene_families, self.transcripts = self.parser.load_transcriptome(transcriptome_path)\n",
    "        \n",
    "        # Analyze gene families\n",
    "        self.gene_family_stats = self.parser.analyze_gene_families(self.gene_families)\n",
    "        \n",
    "        # Load reads if provided\n",
    "        self.reads = None\n",
    "        if reads_path:\n",
    "            self.reads = self._load_reads(reads_path)\n",
    "        \n",
    "        # Load ground truth if provided\n",
    "        self.ground_truth = None\n",
    "        if ground_truth_path:\n",
    "            self.ground_truth = pd.read_csv(ground_truth_path)\n",
    "            self.logger.info(f\"Loaded ground truth with {len(self.ground_truth)} entries\")\n",
    "    \n",
    "    def _load_reads(self, reads_path: str) -> List[str]:\n",
    "        \"\"\"Load sequencing reads from FASTQ file.\"\"\"\n",
    "        self.logger.info(f\"Loading reads from {reads_path}\")\n",
    "        \n",
    "        reads = []\n",
    "        for record in SeqIO.parse(reads_path, \"fastq\"):\n",
    "            reads.append(str(record.seq))\n",
    "        \n",
    "        self.logger.info(f\"Loaded {len(reads)} reads\")\n",
    "        return reads\n",
    "    \n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initialize the neural model and tokenizer.\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Initializing model...\")\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = GeneHierarchicalEncoder(self.config).to(self.device)\n",
    "        \n",
    "        # Initialize loss function\n",
    "        self.contrastive_loss = GeneAwareContrastiveLoss(self.config)\n",
    "        \n",
    "        self.logger.info(\"Model initialized successfully\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the model with gene-aware contrastive learning.\"\"\"\n",
    "        \n",
    "        if not self.model:\n",
    "            self.initialize_model()\n",
    "        \n",
    "        if not self.ground_truth is not None:\n",
    "            raise ValueError(\"Ground truth data required for training\")\n",
    "        \n",
    "        self.logger.info(\"Starting training...\")\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        train_dataset = GeneAwareDataset(\n",
    "            reads=self.reads,\n",
    "            ground_truth=self.ground_truth,\n",
    "            gene_families=self.gene_families,\n",
    "            transcripts=self.transcripts,\n",
    "            config=self.config,\n",
    "            mode=\"train\"\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.config.num_workers,\n",
    "            collate_fn=custom_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Initialize optimizer and scheduler\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        total_steps = len(train_loader) * self.config.num_epochs\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.config.learning_rate,\n",
    "            total_steps=total_steps,\n",
    "            pct_start=0.1\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        training_losses = []\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            epoch_losses = []\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                # Move to device\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                \n",
    "                # Skip batch if no training labels or gene names\n",
    "                if 'true_transcript' not in batch or 'gene_name' not in batch:\n",
    "                    continue\n",
    "                \n",
    "                # Filter out None values in gene_name and true_transcript\n",
    "                valid_indices = []\n",
    "                valid_gene_names = []\n",
    "                valid_transcripts = []\n",
    "                \n",
    "                for i, (gene_name, transcript_id) in enumerate(zip(batch['gene_name'], batch['true_transcript'])):\n",
    "                    if gene_name is not None and transcript_id is not None:\n",
    "                        valid_indices.append(i)\n",
    "                        valid_gene_names.append(gene_name)\n",
    "                        valid_transcripts.append(transcript_id)\n",
    "                \n",
    "                if not valid_indices:\n",
    "                    continue  # Skip batch if no valid samples\n",
    "                \n",
    "                # Filter tensors to valid indices only\n",
    "                input_ids = input_ids[valid_indices]\n",
    "                attention_mask = attention_mask[valid_indices]\n",
    "                \n",
    "                # Create gene and transcript labels for contrastive learning\n",
    "                gene_labels = []\n",
    "                transcript_labels = []\n",
    "                \n",
    "                for gene_name, transcript_id in zip(valid_gene_names, valid_transcripts):\n",
    "                    # Convert to numeric labels (hash for consistency)\n",
    "                    gene_labels.append(hash(gene_name) % 10000)\n",
    "                    transcript_labels.append(hash(transcript_id) % 100000)\n",
    "                \n",
    "                if not gene_labels:  # Skip if no valid labels\n",
    "                    continue\n",
    "                    \n",
    "                gene_labels = torch.tensor(gene_labels, device=self.device)\n",
    "                transcript_labels = torch.tensor(transcript_labels, device=self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                \n",
    "                # Compute contrastive loss\n",
    "                loss_dict = self.contrastive_loss(\n",
    "                    outputs['gene_embedding'],\n",
    "                    outputs['isoform_embedding'],\n",
    "                    gene_labels,\n",
    "                    transcript_labels\n",
    "                )\n",
    "                \n",
    "                total_loss = loss_dict['total_loss']\n",
    "\n",
    "                # Check for invalid loss values\n",
    "                if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "                    self.logger.warning(f\"Invalid loss detected: {total_loss.item()}, skipping batch\")\n",
    "                    continue\n",
    "\n",
    "                # Skip if loss is too large (potential exploding gradient)\n",
    "                if total_loss.item() > 100:\n",
    "                    self.logger.warning(f\"Loss too large: {total_loss.item()}, skipping batch\")\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "\n",
    "                # Add gradient clipping to prevent exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Record loss\n",
    "                epoch_losses.append(total_loss.item())\n",
    "                \n",
    "                # Log progress\n",
    "                if batch_idx % 50 == 0:\n",
    "                    self.logger.info(\n",
    "                        f\"Epoch {epoch+1}/{self.config.num_epochs}, \"\n",
    "                        f\"Batch {batch_idx}/{len(train_loader)}, \"\n",
    "                        f\"Loss: {total_loss.item():.4f}\"\n",
    "                    )\n",
    "            \n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "            training_losses.append(avg_loss)\n",
    "            \n",
    "            self.logger.info(f\"Epoch {epoch+1} completed. Average loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save training stats\n",
    "        self.training_stats = {\n",
    "            'training_losses': training_losses,\n",
    "            'final_loss': training_losses[-1] if training_losses else 0,\n",
    "            'num_epochs': self.config.num_epochs,\n",
    "            'total_batches': len(train_loader) * self.config.num_epochs\n",
    "        }\n",
    "        \n",
    "        self.logger.info(\"Training completed!\")\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build the gene-aware vector search index.\"\"\"\n",
    "        \n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model must be initialized before building index\")\n",
    "        \n",
    "        self.vector_store.build_index(\n",
    "            self.gene_families,\n",
    "            self.transcripts,\n",
    "            self.model,\n",
    "            self.tokenizer\n",
    "        )\n",
    "    \n",
    "    def quantify_reads(self, \n",
    "                      reads: List[str] = None,\n",
    "                      top_k_genes: int = 10,\n",
    "                      top_k_isoforms: int = 5) -> Dict[str, any]:\n",
    "        \"\"\"Quantify reads using gene-aware hierarchical assignment.\"\"\"\n",
    "        \n",
    "        if reads is None:\n",
    "            reads = self.reads\n",
    "        \n",
    "        if not reads:\n",
    "            raise ValueError(\"No reads provided for quantification\")\n",
    "        \n",
    "        if self.vector_store.gene_index is None:\n",
    "            raise ValueError(\"Vector store index must be built before quantification\")\n",
    "        \n",
    "        self.logger.info(f\"Quantifying {len(reads)} reads...\")\n",
    "        \n",
    "        # Results storage\n",
    "        assignments = []\n",
    "        assignment_stats = {\n",
    "            'total_reads': len(reads),\n",
    "            'high_confidence': 0,\n",
    "            'unique_match': 0,\n",
    "            'best_match': 0,\n",
    "            'gene_only': 0,\n",
    "            'unassigned': 0\n",
    "        }\n",
    "        \n",
    "        # Process reads in batches\n",
    "        batch_size = self.config.batch_size\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(reads), batch_size):\n",
    "                batch_reads = reads[i:i+batch_size]\n",
    "                \n",
    "                # Tokenize batch\n",
    "                batch_tokens = []\n",
    "                for read in batch_reads:\n",
    "                    tokens = self.tokenizer(\n",
    "                        read.upper().replace('N', 'A'),\n",
    "                        max_length=self.config.max_sequence_length,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    batch_tokens.append(tokens)\n",
    "                \n",
    "                # Stack tensors\n",
    "                input_ids = torch.stack([t['input_ids'].squeeze(0) for t in batch_tokens]).to(self.device)\n",
    "                attention_mask = torch.stack([t['attention_mask'].squeeze(0) for t in batch_tokens]).to(self.device)\n",
    "                \n",
    "                # Get embeddings\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                \n",
    "                # Process each read in the batch\n",
    "                for j, read in enumerate(batch_reads):\n",
    "                    read_idx = i + j\n",
    "                    \n",
    "                    # Extract embeddings for this read\n",
    "                    gene_emb = outputs['gene_embedding'][j:j+1].cpu().numpy()\n",
    "                    isoform_emb = outputs['isoform_embedding'][j:j+1].cpu().numpy()\n",
    "                    \n",
    "                    # Hierarchical search\n",
    "                    search_results = self.vector_store.hierarchical_search(\n",
    "                        gene_emb, isoform_emb, top_k_genes, top_k_isoforms\n",
    "                    )\n",
    "                    \n",
    "                    # Assignment\n",
    "                    assignment = self.assignment_engine.hierarchical_assignment(search_results)\n",
    "                    \n",
    "                    # Record assignment\n",
    "                    assignment['read_id'] = f\"read_{read_idx:06d}\"\n",
    "                    assignment['read_sequence'] = read\n",
    "                    assignments.append(assignment)\n",
    "                    \n",
    "                    # Update stats\n",
    "                    assignment_stats[assignment['assignment_type']] += 1\n",
    "                \n",
    "                # Log progress\n",
    "                if i % (batch_size * 10) == 0:\n",
    "                    self.logger.info(f\"Processed {i+len(batch_reads)}/{len(reads)} reads\")\n",
    "        \n",
    "        self.logger.info(\"Quantification completed!\")\n",
    "        self.logger.info(f\"Assignment statistics: {assignment_stats}\")\n",
    "        \n",
    "        return {\n",
    "            'assignments': assignments,\n",
    "            'stats': assignment_stats,\n",
    "            'transcript_counts': self._compute_transcript_counts(assignments)\n",
    "        }\n",
    "    \n",
    "    def _compute_transcript_counts(self, assignments: List[Dict]) -> Dict[str, Dict]:\n",
    "        \"\"\"Compute transcript abundance estimates from assignments.\"\"\"\n",
    "        \n",
    "        transcript_counts = defaultdict(lambda: {\n",
    "            'raw_count': 0,\n",
    "            'weighted_count': 0.0,\n",
    "            'confidence_sum': 0.0,\n",
    "            'assignments': []\n",
    "        })\n",
    "        \n",
    "        for assignment in assignments:\n",
    "            if assignment['assigned_transcript']:\n",
    "                tid = assignment['assigned_transcript']\n",
    "                confidence = assignment['confidence']\n",
    "                \n",
    "                transcript_counts[tid]['raw_count'] += 1\n",
    "                transcript_counts[tid]['weighted_count'] += confidence\n",
    "                transcript_counts[tid]['confidence_sum'] += confidence\n",
    "                transcript_counts[tid]['assignments'].append(assignment)\n",
    "        \n",
    "        # Compute final metrics\n",
    "        final_counts = {}\n",
    "        total_weighted = sum(tc['weighted_count'] for tc in transcript_counts.values())\n",
    "        \n",
    "        for tid, counts in transcript_counts.items():\n",
    "            num_assignments = len(counts['assignments'])\n",
    "            avg_confidence = counts['confidence_sum'] / num_assignments if num_assignments > 0 else 0.0\n",
    "            \n",
    "            final_counts[tid] = {\n",
    "                'raw_count': counts['raw_count'],\n",
    "                'weighted_count': counts['weighted_count'],\n",
    "                'tpm': (counts['weighted_count'] / total_weighted * 1e6) if total_weighted > 0 else 0.0,\n",
    "                'average_confidence': avg_confidence,\n",
    "                'gene_name': self.transcripts[tid].gene_name if tid in self.transcripts else 'unknown'\n",
    "            }\n",
    "        \n",
    "        return final_counts\n",
    "    \n",
    "    def save_results(self, results: Dict[str, any], output_prefix: str = \"deepquant_v2\"):\n",
    "        \"\"\"Save quantification results to files.\"\"\"\n",
    "        \n",
    "        output_dir = Path(self.config.output_dir)\n",
    "        \n",
    "        # Save assignments\n",
    "        assignments_df = pd.DataFrame(results['assignments'])\n",
    "        assignments_file = output_dir / f\"{output_prefix}_assignments.csv\"\n",
    "        assignments_df.to_csv(assignments_file, index=False)\n",
    "        \n",
    "        # Save transcript counts\n",
    "        counts_data = []\n",
    "        for tid, counts in results['transcript_counts'].items():\n",
    "            counts_data.append({\n",
    "                'transcript_id': tid,\n",
    "                'gene_name': counts['gene_name'],\n",
    "                'raw_count': counts['raw_count'],\n",
    "                'weighted_count': counts['weighted_count'],\n",
    "                'tpm': counts['tpm'],\n",
    "                'average_confidence': counts['average_confidence']\n",
    "            })\n",
    "        \n",
    "        counts_df = pd.DataFrame(counts_data).sort_values('weighted_count', ascending=False)\n",
    "        counts_file = output_dir / f\"{output_prefix}_transcript_counts.csv\"\n",
    "        counts_df.to_csv(counts_file, index=False)\n",
    "        \n",
    "        # Save statistics\n",
    "        stats_file = output_dir / f\"{output_prefix}_stats.json\"\n",
    "        with open(stats_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'assignment_stats': results['stats'],\n",
    "                'gene_family_stats': self.gene_family_stats,\n",
    "                'training_stats': self.training_stats,\n",
    "                'config': self.config.__dict__\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Results saved to {output_dir}\")\n",
    "        self.logger.info(f\"  Assignments: {assignments_file}\")\n",
    "        self.logger.info(f\"  Counts: {counts_file}\")\n",
    "        self.logger.info(f\"  Stats: {stats_file}\")\n",
    "        \n",
    "        return {\n",
    "            'assignments_file': assignments_file,\n",
    "            'counts_file': counts_file,\n",
    "            'stats_file': stats_file\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6cf0ec",
   "metadata": {},
   "source": [
    "### Evaluation and Validation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da404799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQuantEvaluator:\n",
    "    \"\"\"Comprehensive evaluation framework for DeepQuant V2.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def evaluate_against_ground_truth(self,\n",
    "                                    assignments: List[Dict],\n",
    "                                    ground_truth: pd.DataFrame) -> Dict[str, any]:\n",
    "        \"\"\"Evaluate assignments against ground truth.\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Evaluating against ground truth...\")\n",
    "        \n",
    "        # Align assignments with ground truth\n",
    "        eval_data = []\n",
    "        \n",
    "        for i, assignment in enumerate(assignments):\n",
    "            if i < len(ground_truth):\n",
    "                gt_row = ground_truth.iloc[i]\n",
    "                \n",
    "                eval_data.append({\n",
    "                    'read_id': assignment['read_id'],\n",
    "                    'predicted_transcript': assignment.get('assigned_transcript'),\n",
    "                    'predicted_gene': assignment.get('assigned_gene'), \n",
    "                    'true_transcript': gt_row['true_transcript'],\n",
    "                    'true_gene': gt_row.get('true_gene', 'unknown'),\n",
    "                    'confidence': assignment.get('confidence', 0.0),\n",
    "                    'assignment_type': assignment.get('assignment_type', 'unknown')\n",
    "                })\n",
    "        \n",
    "        eval_df = pd.DataFrame(eval_data)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = self._compute_evaluation_metrics(eval_df)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _compute_evaluation_metrics(self, eval_df: pd.DataFrame) -> Dict[str, any]:\n",
    "        \"\"\"Compute comprehensive evaluation metrics.\"\"\"\n",
    "        \n",
    "        total_reads = len(eval_df)\n",
    "        \n",
    "        # Basic assignment metrics\n",
    "        assigned_reads = eval_df['predicted_transcript'].notna().sum()\n",
    "        assignment_rate = assigned_reads / total_reads\n",
    "        \n",
    "        # Transcript-level accuracy\n",
    "        transcript_correct = (eval_df['predicted_transcript'] == eval_df['true_transcript']).sum()\n",
    "        transcript_accuracy = transcript_correct / total_reads\n",
    "        \n",
    "        # Gene-level accuracy (including partial assignments)\n",
    "        gene_correct = (eval_df['predicted_gene'] == eval_df['true_gene']).sum()\n",
    "        gene_accuracy = gene_correct / total_reads\n",
    "        \n",
    "        # Confidence-weighted metrics\n",
    "        assigned_df = eval_df[eval_df['predicted_transcript'].notna()].copy()\n",
    "        if len(assigned_df) > 0:\n",
    "            weighted_transcript_acc = ((assigned_df['predicted_transcript'] == assigned_df['true_transcript']) * \n",
    "                                     assigned_df['confidence']).sum() / assigned_df['confidence'].sum()\n",
    "            avg_confidence = assigned_df['confidence'].mean()\n",
    "        else:\n",
    "            weighted_transcript_acc = 0.0\n",
    "            avg_confidence = 0.0\n",
    "        \n",
    "        # Assignment type breakdown\n",
    "        assignment_breakdown = eval_df['assignment_type'].value_counts().to_dict()\n",
    "        \n",
    "        # Confidence analysis\n",
    "        confidence_stats = {\n",
    "            'mean': float(eval_df['confidence'].mean()),\n",
    "            'std': float(eval_df['confidence'].std()),\n",
    "            'median': float(eval_df['confidence'].median()),\n",
    "            'q75': float(eval_df['confidence'].quantile(0.75)),\n",
    "            'q25': float(eval_df['confidence'].quantile(0.25))\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'total_reads': total_reads,\n",
    "            'assignment_rate': assignment_rate,\n",
    "            'transcript_accuracy': transcript_accuracy,\n",
    "            'gene_accuracy': gene_accuracy,\n",
    "            'weighted_transcript_accuracy': weighted_transcript_acc,\n",
    "            'average_confidence': avg_confidence,\n",
    "            'assignment_breakdown': assignment_breakdown,\n",
    "            'confidence_stats': confidence_stats\n",
    "        }\n",
    "    \n",
    "    def generate_evaluation_report(self, metrics: Dict[str, any], output_dir: str):\n",
    "        \"\"\"Generate comprehensive evaluation report with visualizations.\"\"\"\n",
    "        \n",
    "        output_path = Path(output_dir)\n",
    "        \n",
    "        # Create evaluation report\n",
    "        report = {\n",
    "            'summary': {\n",
    "                'assignment_rate': f\"{metrics['assignment_rate']:.1%}\",\n",
    "                'transcript_accuracy': f\"{metrics['transcript_accuracy']:.1%}\",\n",
    "                'gene_accuracy': f\"{metrics['gene_accuracy']:.1%}\",\n",
    "                'average_confidence': f\"{metrics['average_confidence']:.3f}\"\n",
    "            },\n",
    "            'detailed_metrics': metrics\n",
    "        }\n",
    "        \n",
    "        # Save report\n",
    "        report_file = output_path / \"evaluation_report.json\"\n",
    "        with open(report_file, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Evaluation report saved to {report_file}\")\n",
    "        \n",
    "        return report_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755750da",
   "metadata": {},
   "source": [
    "### Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc57725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 20:01:20,835 - __main__ - INFO - Loading transcriptome from /run/media/saadat/A/tools/DeepQuant/gencode.v47.transcripts_1000.fa\n",
      "2025-08-25 20:01:20,860 - __main__ - INFO - Loaded 1000 transcripts, 61 gene families\n",
      "2025-08-25 20:01:20,860 - __main__ - INFO - Skipped 0 transcripts\n",
      "2025-08-25 20:01:20,861 - __main__ - INFO - Analyzing gene family complexity...\n",
      "2025-08-25 20:01:20,861 - __main__ - INFO - Gene family analysis complete:\n",
      "2025-08-25 20:01:20,861 - __main__ - INFO -   Total genes: 61\n",
      "2025-08-25 20:01:20,862 - __main__ - INFO -   Single isoform genes: 43\n",
      "2025-08-25 20:01:20,862 - __main__ - INFO -   Multi-isoform genes: 18\n",
      "2025-08-25 20:01:20,862 - __main__ - INFO -   Most complex gene: 332 isoforms\n",
      "2025-08-25 20:01:20,863 - __main__ - INFO - Loading reads from /run/media/saadat/A/tools/DeepQuant/simulated_reads.fastq\n",
      "2025-08-25 20:01:20,910 - __main__ - INFO - Loaded 10000 reads\n",
      "2025-08-25 20:01:20,930 - __main__ - INFO - Loaded ground truth with 10000 entries\n",
      "2025-08-25 20:01:20,930 - __main__ - INFO - Initializing model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧬 Loading transcriptome and data...\n",
      "🤖 Initializing and training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 20:01:23.607105: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-25 20:01:23.618871: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756144883.631121   11972 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756144883.635294   11972 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756144883.645022   11972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756144883.645034   11972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756144883.645035   11972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756144883.645036   11972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-25 20:01:23.648146: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-25 20:01:25,696 - __main__ - INFO - Model initialized successfully\n",
      "2025-08-25 20:01:25,697 - __main__ - INFO - Starting training...\n",
      "2025-08-25 20:01:26,984 - __main__ - INFO - Epoch 1/5, Batch 0/1250, Loss: 0.6099\n",
      "2025-08-25 20:01:52,580 - __main__ - INFO - Epoch 1/5, Batch 50/1250, Loss: 0.3929\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 7.61 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 6.81 GiB memory in use. Of the allocated memory 6.42 GiB is allocated by PyTorch, and 238.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dq, results, eval_metrics\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 36\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🤖 Initializing and training model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m dq\u001b[38;5;241m.\u001b[39minitialize_model()\n\u001b[0;32m---> 36\u001b[0m \u001b[43mdq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Build search index\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔍 Building gene-aware search index...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 174\u001b[0m, in \u001b[0;36mDeepQuantV2.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m transcript_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(transcript_labels, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Compute contrastive loss\u001b[39;00m\n\u001b[1;32m    177\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrastive_loss(\n\u001b[1;32m    178\u001b[0m     outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgene_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m     outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misoform_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m     gene_labels,\n\u001b[1;32m    181\u001b[0m     transcript_labels\n\u001b[1;32m    182\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 131\u001b[0m, in \u001b[0;36mGeneHierarchicalEncoder.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Base encoding\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     base_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m base_output\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Multi-scale attention\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1006\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1006\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1020\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:653\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    649\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m    651\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 653\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/transformers/modeling_layers.py:93\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:592\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    589\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    590\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/transformers/pytorch_utils.py:251\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:600\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 600\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:518\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    517\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 518\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/tabpfn/lib/python3.9/site-packages/transformers/activations.py:69\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 7.61 GiB of which 50.06 MiB is free. Including non-PyTorch memory, this process has 6.81 GiB memory in use. Of the allocated memory 6.42 GiB is allocated by PyTorch, and 238.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function for DeepQuant V2.\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    config = DeepQuantConfig(\n",
    "        # Model parameters\n",
    "        embedding_dim=256,\n",
    "        gene_embedding_dim=128,\n",
    "        batch_size=8,  # Reduced for memory efficiency\n",
    "        learning_rate=1e-5,\n",
    "        num_epochs=5,\n",
    "        \n",
    "        # Assignment parameters\n",
    "        gene_similarity_threshold=0.7,\n",
    "        isoform_similarity_threshold=0.85,\n",
    "        uncertainty_threshold=0.8,\n",
    "        \n",
    "        # Paths\n",
    "        output_dir=\"./deepquant_v2_results\"\n",
    "    )\n",
    "    \n",
    "    # Initialize DeepQuant V2\n",
    "    dq = DeepQuantV2(config)\n",
    "    \n",
    "    # Load data\n",
    "    transcriptome_path = \"/run/media/saadat/A/tools/DeepQuant/gencode.v47.transcripts_1000.fa\"\n",
    "    reads_path = \"/run/media/saadat/A/tools/DeepQuant/simulated_reads.fastq\"\n",
    "    ground_truth_path = \"/run/media/saadat/A/tools/DeepQuant/simulated_ground_truth.csv\"\n",
    "    \n",
    "    print(\"🧬 Loading transcriptome and data...\")\n",
    "    dq.load_data(transcriptome_path, reads_path, ground_truth_path)\n",
    "    \n",
    "    # Initialize and train model\n",
    "    print(\"🤖 Initializing and training model...\")\n",
    "    dq.initialize_model()\n",
    "    dq.train()\n",
    "    \n",
    "    # Build search index\n",
    "    print(\"🔍 Building gene-aware search index...\")\n",
    "    dq.build_index()\n",
    "    \n",
    "    # Quantify reads\n",
    "    print(\"📊 Quantifying reads...\")\n",
    "    results = dq.quantify_reads()\n",
    "    \n",
    "    # Save results\n",
    "    print(\"💾 Saving results...\")\n",
    "    output_files = dq.save_results(results)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"📈 Evaluating results...\")\n",
    "    evaluator = DeepQuantEvaluator(config)\n",
    "    eval_metrics = evaluator.evaluate_against_ground_truth(\n",
    "        results['assignments'], \n",
    "        dq.ground_truth\n",
    "    )\n",
    "    \n",
    "    # Generate evaluation report\n",
    "    evaluator.generate_evaluation_report(eval_metrics, config.output_dir)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 DEEPQUANT V2 RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"📋 Total reads processed: {eval_metrics['total_reads']}\")\n",
    "    print(f\"✅ Assignment rate: {eval_metrics['assignment_rate']:.1%}\")\n",
    "    print(f\"🎯 Transcript accuracy: {eval_metrics['transcript_accuracy']:.1%}\")\n",
    "    print(f\"🧬 Gene accuracy: {eval_metrics['gene_accuracy']:.1%}\")\n",
    "    print(f\"🎲 Average confidence: {eval_metrics['average_confidence']:.3f}\")\n",
    "    print(f\"📁 Results saved to: {config.output_dir}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return dq, results, eval_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabpfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
