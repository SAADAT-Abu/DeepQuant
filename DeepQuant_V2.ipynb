{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b94d5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDeepQuant V2: Gene-Aware Transcriptome Quantification with Hierarchical Learning\\n================================================================================\\n\\nA next-generation transcriptome quantification tool that leverages gene family relationships\\nfor improved isoform disambiguation using transformer architectures and probabilistic modeling.\\n\\nKey Features:\\n- Gene-aware hierarchical modeling\\n- Multi-level contrastive learning  \\n- Uncertainty-aware assignment strategies\\n- Integration with classical statistical methods\\n- Comprehensive evaluation framework\\n\\nVersion: 2.0\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DeepQuant V2: Gene-Aware Transcriptome Quantification with Hierarchical Learning\n",
    "================================================================================\n",
    "\n",
    "A next-generation transcriptome quantification tool that leverages gene family relationships\n",
    "for improved isoform disambiguation using transformer architectures and probabilistic modeling.\n",
    "\n",
    "Key Features:\n",
    "- Gene-aware hierarchical modeling\n",
    "- Multi-level contrastive learning  \n",
    "- Uncertainty-aware assignment strategies\n",
    "- Integration with classical statistical methods\n",
    "- Comprehensive evaluation framework\n",
    "\n",
    "Version: 2.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2674f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import faiss\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from scipy.stats import entropy\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf34a1a9",
   "metadata": {},
   "source": [
    "### Configuration and Data Structures  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fca9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DeepQuantConfig:\n",
    "    \"\"\"Configuration class for DeepQuant V2.\"\"\"\n",
    "    \n",
    "    # Model architecture\n",
    "    model_name: str = \"zhihan1996/DNABERT-2-117M\"\n",
    "    embedding_dim: int = 256\n",
    "    gene_embedding_dim: int = 128\n",
    "    dropout: float = 0.1\n",
    "    num_attention_heads: int = 8\n",
    "    hidden_dim_multiplier: int = 4\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    num_epochs: int = 10\n",
    "    warmup_steps: int = 1000\n",
    "    \n",
    "    # Gene-aware learning\n",
    "    gene_contrastive_weight: float = 0.3\n",
    "    isoform_contrastive_weight: float = 0.7\n",
    "    gene_hierarchy_weight: float = 0.2\n",
    "    temperature: float = 0.1\n",
    "    \n",
    "    # Assignment strategy\n",
    "    assignment_strategy: str = \"hierarchical\"  # \"hierarchical\", \"joint\", \"ensemble\"\n",
    "    gene_similarity_threshold: float = 0.7\n",
    "    isoform_similarity_threshold: float = 0.85\n",
    "    uncertainty_threshold: float = 0.8\n",
    "    \n",
    "    # Computational\n",
    "    max_sequence_length: int = 512\n",
    "    use_mixed_precision: bool = True\n",
    "    num_workers: int = 4\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Output and logging\n",
    "    output_dir: str = \"./deepquant_v2_results\"\n",
    "    log_level: str = \"INFO\"\n",
    "    save_attention_maps: bool = False\n",
    "\n",
    "@dataclass \n",
    "class TranscriptInfo:\n",
    "    \"\"\"Information about a single transcript.\"\"\"\n",
    "    transcript_id: str\n",
    "    gene_id: str\n",
    "    gene_name: str\n",
    "    sequence: str\n",
    "    length: int\n",
    "    transcript_type: str\n",
    "    \n",
    "@dataclass\n",
    "class GeneFamilyInfo:\n",
    "    \"\"\"Information about a gene family and its isoforms.\"\"\"\n",
    "    gene_name: str\n",
    "    gene_id: str\n",
    "    transcripts: Dict[str, TranscriptInfo]\n",
    "    similarity_matrix: Optional[np.ndarray] = None\n",
    "    complexity_score: float = 0.0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.num_isoforms = len(self.transcripts)\n",
    "        self.transcript_ids = list(self.transcripts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f68d1",
   "metadata": {},
   "source": [
    "### Data Loading and Gene Family Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9de192e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptomeParser:\n",
    "    \"\"\"Parse transcriptome FASTA and organize by gene families.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def parse_fasta_header(self, header: str) -> Dict[str, str]:\n",
    "        \"\"\"Parse GENCODE-style FASTA header - extract only essential fields.\"\"\"\n",
    "        # Remove '>' and split by '|'\n",
    "        fields = header.split('|')\n",
    "        \n",
    "        if len(fields) < 6:  # Need at least transcript_id and gene_name\n",
    "            self.logger.warning(f\"Header missing essential fields: {header}\")\n",
    "            return None\n",
    "            \n",
    "        return {\n",
    "            'transcript_id': fields[0],          # First field: transcript ID\n",
    "            'gene_name': fields[5],              # Sixth field: gene name  \n",
    "            'gene_id': fields[1] if len(fields) > 1 else fields[0],  # Fallback to transcript_id\n",
    "            'transcript_type': fields[7] if len(fields) > 7 else 'unknown'  # Optional\n",
    "        }\n",
    "    \n",
    "    def load_transcriptome(self, fasta_path: str) -> Tuple[Dict[str, GeneFamilyInfo], Dict[str, TranscriptInfo]]:\n",
    "        \"\"\"Load transcriptome and organize by gene families.\"\"\"\n",
    "        self.logger.info(f\"Loading transcriptome from {fasta_path}\")\n",
    "        \n",
    "        transcripts = {}\n",
    "        gene_families = defaultdict(lambda: {\n",
    "            'gene_name': '',\n",
    "            'gene_id': '',\n",
    "            'transcripts': {}\n",
    "        })\n",
    "        \n",
    "        skipped = 0\n",
    "        loaded = 0\n",
    "        \n",
    "        try:\n",
    "            for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "                header_info = self.parse_fasta_header(record.id)\n",
    "                if not header_info:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Skip very short sequences\n",
    "                if len(record.seq) < 50:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                transcript_info = TranscriptInfo(\n",
    "                    transcript_id=header_info['transcript_id'],\n",
    "                    gene_id=header_info['gene_id'],\n",
    "                    gene_name=header_info['gene_name'],\n",
    "                    sequence=str(record.seq).upper(),\n",
    "                    length=len(record.seq),\n",
    "                    transcript_type=header_info['transcript_type']\n",
    "                )\n",
    "                \n",
    "                transcripts[transcript_info.transcript_id] = transcript_info\n",
    "                \n",
    "                # Group by gene name\n",
    "                gene_name = header_info['gene_name']\n",
    "                gene_families[gene_name]['gene_name'] = gene_name\n",
    "                gene_families[gene_name]['gene_id'] = header_info['gene_id']\n",
    "                gene_families[gene_name]['transcripts'][transcript_info.transcript_id] = transcript_info\n",
    "                \n",
    "                loaded += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading transcriptome: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Convert to GeneFamilyInfo objects\n",
    "        final_gene_families = {}\n",
    "        for gene_name, info in gene_families.items():\n",
    "            final_gene_families[gene_name] = GeneFamilyInfo(\n",
    "                gene_name=info['gene_name'],\n",
    "                gene_id=info['gene_id'],\n",
    "                transcripts=info['transcripts']\n",
    "            )\n",
    "        \n",
    "        self.logger.info(f\"Loaded {loaded} transcripts, {len(final_gene_families)} gene families\")\n",
    "        self.logger.info(f\"Skipped {skipped} transcripts\")\n",
    "        \n",
    "        return final_gene_families, transcripts\n",
    "    \n",
    "    def analyze_gene_families(self, gene_families: Dict[str, GeneFamilyInfo]) -> Dict[str, any]:\n",
    "        \"\"\"Analyze gene family complexity and similarity patterns.\"\"\"\n",
    "        self.logger.info(\"Analyzing gene family complexity...\")\n",
    "        \n",
    "        stats = {\n",
    "            'total_genes': len(gene_families),\n",
    "            'total_transcripts': sum(len(gf.transcripts) for gf in gene_families.values()),\n",
    "            'single_isoform_genes': 0,\n",
    "            'multi_isoform_genes': 0,\n",
    "            'max_isoforms': 0,\n",
    "            'complex_genes': [],\n",
    "            'isoform_distribution': Counter()\n",
    "        }\n",
    "        \n",
    "        for gene_name, gene_family in gene_families.items():\n",
    "            num_isoforms = len(gene_family.transcripts)\n",
    "            stats['isoform_distribution'][num_isoforms] += 1\n",
    "            \n",
    "            if num_isoforms == 1:\n",
    "                stats['single_isoform_genes'] += 1\n",
    "            else:\n",
    "                stats['multi_isoform_genes'] += 1\n",
    "                \n",
    "            if num_isoforms > stats['max_isoforms']:\n",
    "                stats['max_isoforms'] = num_isoforms\n",
    "            \n",
    "            # Mark complex genes (>5 isoforms) for special attention\n",
    "            if num_isoforms > 5:\n",
    "                stats['complex_genes'].append((gene_name, num_isoforms))\n",
    "        \n",
    "        self.logger.info(f\"Gene family analysis complete:\")\n",
    "        self.logger.info(f\"  Total genes: {stats['total_genes']}\")\n",
    "        self.logger.info(f\"  Single isoform genes: {stats['single_isoform_genes']}\")\n",
    "        self.logger.info(f\"  Multi-isoform genes: {stats['multi_isoform_genes']}\")\n",
    "        self.logger.info(f\"  Most complex gene: {stats['max_isoforms']} isoforms\")\n",
    "        \n",
    "        return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc58fa5",
   "metadata": {},
   "source": [
    "### Gene-Aware Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f089387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneAwareDataset(Dataset):\n",
    "    \"\"\"Dataset that provides gene family context for training.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 reads: List[str],\n",
    "                 ground_truth: pd.DataFrame,\n",
    "                 gene_families: Dict[str, GeneFamilyInfo],\n",
    "                 transcripts: Dict[str, TranscriptInfo],\n",
    "                 config: DeepQuantConfig,\n",
    "                 mode: str = \"train\"):\n",
    "        \n",
    "        self.reads = reads\n",
    "        self.ground_truth = ground_truth\n",
    "        self.gene_families = gene_families\n",
    "        self.transcripts = transcripts\n",
    "        self.config = config\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Create mappings\n",
    "        self.transcript_to_gene = {t_id: info.gene_name \n",
    "                                  for t_id, info in transcripts.items()}\n",
    "        \n",
    "        # Prepare tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        \n",
    "        # Create gene family lookup for efficient sampling\n",
    "        self.gene_to_transcripts = defaultdict(list)\n",
    "        for t_id, info in transcripts.items():\n",
    "            self.gene_to_transcripts[info.gene_name].append(t_id)\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reads)\n",
    "    \n",
    "    def tokenize_sequence(self, sequence: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Tokenize DNA sequence with proper truncation/padding.\"\"\"\n",
    "        sequence = sequence.upper().replace('N', 'A')\n",
    "        \n",
    "        tokens = self.tokenizer(\n",
    "            sequence,\n",
    "            max_length=self.config.max_sequence_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        return {k: v.squeeze(0) for k, v in tokens.items()}\n",
    "    \n",
    "    def get_gene_family_context(self, transcript_id: str) -> Dict[str, any]:\n",
    "        \"\"\"Get gene family context for a transcript.\"\"\"\n",
    "        transcript_info = self.transcripts[transcript_id]\n",
    "        gene_name = transcript_info.gene_name\n",
    "        gene_family = self.gene_families[gene_name]\n",
    "        \n",
    "        # Get all isoforms in the same gene family\n",
    "        family_transcripts = list(gene_family.transcripts.keys())\n",
    "        \n",
    "        # Sample negative transcripts from different genes\n",
    "        negative_genes = [g for g in self.gene_families.keys() if g != gene_name]\n",
    "        negative_transcripts = []\n",
    "        if negative_genes:\n",
    "            neg_gene = np.random.choice(negative_genes)\n",
    "            neg_family = self.gene_families[neg_gene]\n",
    "            negative_transcripts = list(neg_family.transcripts.keys())\n",
    "        \n",
    "        return {\n",
    "            'gene_name': gene_name,\n",
    "            'family_transcripts': family_transcripts,\n",
    "            'negative_transcripts': negative_transcripts,\n",
    "            'num_isoforms': len(family_transcripts),\n",
    "            'is_complex_gene': len(family_transcripts) > 3\n",
    "        }\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        read_sequence = self.reads[idx]\n",
    "        \n",
    "        # Get ground truth information\n",
    "        if self.ground_truth is not None and idx < len(self.ground_truth):\n",
    "            gt_row = self.ground_truth.iloc[idx]\n",
    "            true_transcript = gt_row['true_transcript']\n",
    "        else:\n",
    "            # For inference mode\n",
    "            true_transcript = None\n",
    "        \n",
    "        # Tokenize the read\n",
    "        read_tokens = self.tokenize_sequence(read_sequence)\n",
    "        \n",
    "        sample = {\n",
    "            'read_id': f\"read_{idx:06d}\",\n",
    "            'read_sequence': read_sequence,\n",
    "            'input_ids': read_tokens['input_ids'],\n",
    "            'attention_mask': read_tokens['attention_mask'],\n",
    "            'read_idx': idx\n",
    "        }\n",
    "        \n",
    "        # Add training-specific information\n",
    "        if self.mode == \"train\" and true_transcript is not None:\n",
    "            sample['true_transcript'] = true_transcript\n",
    "            \n",
    "            # Get gene family context\n",
    "            if true_transcript in self.transcripts:\n",
    "                try:\n",
    "                    gene_context = self.get_gene_family_context(true_transcript)\n",
    "                    sample.update({\n",
    "                        'gene_name': gene_context['gene_name'],\n",
    "                        'family_transcripts': gene_context['family_transcripts'],\n",
    "                        'negative_transcripts': gene_context['negative_transcripts'],\n",
    "                        'num_isoforms': gene_context['num_isoforms'],\n",
    "                        'is_complex_gene': gene_context['is_complex_gene']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error getting gene context for {true_transcript}: {e}\")\n",
    "                    # Provide fallback values\n",
    "                    sample.update({\n",
    "                        'gene_name': None,\n",
    "                        'family_transcripts': [],\n",
    "                        'negative_transcripts': [],\n",
    "                        'num_isoforms': 1,\n",
    "                        'is_complex_gene': False\n",
    "                    })\n",
    "            else:\n",
    "                self.logger.warning(f\"Unknown transcript in ground truth: {true_transcript}\")\n",
    "                # Provide fallback values\n",
    "                sample.update({\n",
    "                    'gene_name': None,\n",
    "                    'family_transcripts': [],\n",
    "                    'negative_transcripts': [],\n",
    "                    'num_isoforms': 1,\n",
    "                    'is_complex_gene': False\n",
    "                })\n",
    "        \n",
    "        return sample\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom collate function that handles variable-length gene family information.\"\"\"\n",
    "    \n",
    "    # Tensor fields that are already tensors\n",
    "    tensor_fields = ['input_ids', 'attention_mask']\n",
    "    collated = {}\n",
    "    \n",
    "    for field in tensor_fields:\n",
    "        if field in batch[0]:\n",
    "            collated[field] = torch.stack([sample[field] for sample in batch])\n",
    "    \n",
    "    # Handle read_idx separately (it's an integer, needs to be converted to tensor)\n",
    "    if 'read_idx' in batch[0]:\n",
    "        collated['read_idx'] = torch.tensor([sample['read_idx'] for sample in batch])\n",
    "    \n",
    "    # List fields\n",
    "    list_fields = ['read_id', 'read_sequence', 'true_transcript', 'gene_name']\n",
    "    for field in list_fields:\n",
    "        if field in batch[0]:\n",
    "            collated[field] = [sample[field] for sample in batch]\n",
    "    \n",
    "    # Gene family context (variable length)\n",
    "    if 'family_transcripts' in batch[0]:\n",
    "        collated['family_transcripts'] = [sample['family_transcripts'] for sample in batch]\n",
    "        collated['negative_transcripts'] = [sample['negative_transcripts'] for sample in batch]\n",
    "        collated['num_isoforms'] = torch.tensor([sample['num_isoforms'] for sample in batch])\n",
    "        collated['is_complex_gene'] = torch.tensor([sample['is_complex_gene'] for sample in batch])\n",
    "    \n",
    "    return collated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795fe16",
   "metadata": {},
   "source": [
    "### Neural Architecture Components  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd539119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleAttention(nn.Module):\n",
    "    \"\"\"Multi-scale attention mechanism for capturing both local and global patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Multi-scale attention heads\n",
    "        self.local_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads // 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.global_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads // 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.scale_fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        \n",
    "        # Convert attention mask to key_padding_mask format if provided\n",
    "        key_padding_mask = None\n",
    "        if attention_mask is not None:\n",
    "            # attention_mask: 1 for real tokens, 0 for padding\n",
    "            # key_padding_mask: True for padding tokens, False for real tokens\n",
    "            key_padding_mask = (attention_mask == 0)\n",
    "        \n",
    "        # Local attention with restricted window\n",
    "        local_out, local_weights = self.local_attention(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        # Global attention \n",
    "        global_out, global_weights = self.global_attention(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        \n",
    "        # Fuse multi-scale features\n",
    "        fused = torch.cat([local_out, global_out], dim=-1)\n",
    "        output = self.scale_fusion(fused)\n",
    "        \n",
    "        return output, {'local_weights': local_weights, 'global_weights': global_weights}\n",
    "\n",
    "class GeneHierarchicalEncoder(nn.Module):\n",
    "    \"\"\"Hierarchical encoder that learns gene-level and isoform-level representations.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Base transformer encoder\n",
    "        self.base_encoder = AutoModel.from_pretrained(config.model_name)\n",
    "        base_hidden_size = self.base_encoder.config.hidden_size\n",
    "        \n",
    "        # Gene-level encoder\n",
    "        self.gene_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=base_hidden_size,\n",
    "                nhead=config.num_attention_heads,\n",
    "                dim_feedforward=base_hidden_size * config.hidden_dim_multiplier,\n",
    "                dropout=config.dropout,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Isoform-specific encoder\n",
    "        self.isoform_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=base_hidden_size,\n",
    "                nhead=config.num_attention_heads,\n",
    "                dim_feedforward=base_hidden_size * config.hidden_dim_multiplier,\n",
    "                dropout=config.dropout,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Multi-scale attention\n",
    "        self.multi_scale_attention = MultiScaleAttention(\n",
    "            hidden_dim=base_hidden_size,\n",
    "            num_heads=config.num_attention_heads,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        \n",
    "        # Projection heads\n",
    "        self.gene_projection = nn.Sequential(\n",
    "            nn.Linear(base_hidden_size, config.gene_embedding_dim * 2),\n",
    "            nn.LayerNorm(config.gene_embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.gene_embedding_dim * 2, config.gene_embedding_dim),\n",
    "            nn.LayerNorm(config.gene_embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.isoform_projection = nn.Sequential(\n",
    "            nn.Linear(base_hidden_size, config.embedding_dim * 2),\n",
    "            nn.LayerNorm(config.embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.embedding_dim * 2, config.embedding_dim),\n",
    "            nn.LayerNorm(config.embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Uncertainty estimation heads\n",
    "        self.gene_uncertainty = nn.Sequential(\n",
    "            nn.Linear(config.gene_embedding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.isoform_uncertainty = nn.Sequential(\n",
    "            nn.Linear(config.embedding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Base encoding\n",
    "        base_output = self.base_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = base_output.last_hidden_state\n",
    "        \n",
    "        # Multi-scale attention\n",
    "        enhanced_output, attention_weights = self.multi_scale_attention(\n",
    "            sequence_output, attention_mask\n",
    "        )\n",
    "        \n",
    "        # Convert attention mask to proper format for transformer layers\n",
    "        # attention_mask: 1 for real tokens, 0 for padding\n",
    "        # key_padding_mask: True for padding tokens, False for real tokens\n",
    "        padding_mask = (attention_mask == 0)  # Convert to boolean mask\n",
    "\n",
    "        # Gene-level encoding (captures common patterns)\n",
    "        gene_features = self.gene_encoder(enhanced_output, src_key_padding_mask=padding_mask)\n",
    "\n",
    "        # Isoform-level encoding (captures discriminative patterns) \n",
    "        isoform_features = self.isoform_encoder(enhanced_output, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Global pooling with attention mask\n",
    "        gene_pooled = self._attention_pooling(gene_features, attention_mask)\n",
    "        isoform_pooled = self._attention_pooling(isoform_features, attention_mask)\n",
    "        \n",
    "        # Project to embedding spaces\n",
    "        gene_embedding = self.gene_projection(gene_pooled)\n",
    "        isoform_embedding = self.isoform_projection(isoform_pooled)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        gene_embedding = F.normalize(gene_embedding, p=2, dim=-1)\n",
    "        isoform_embedding = F.normalize(isoform_embedding, p=2, dim=-1)\n",
    "        \n",
    "        # Uncertainty estimates\n",
    "        gene_uncertainty = self.gene_uncertainty(gene_embedding)\n",
    "        isoform_uncertainty = self.isoform_uncertainty(isoform_embedding)\n",
    "        \n",
    "        return {\n",
    "            'gene_embedding': gene_embedding,\n",
    "            'isoform_embedding': isoform_embedding,\n",
    "            'gene_uncertainty': gene_uncertainty,\n",
    "            'isoform_uncertainty': isoform_uncertainty,\n",
    "            'attention_weights': attention_weights\n",
    "        }\n",
    "    \n",
    "    def _attention_pooling(self, features, attention_mask):\n",
    "        \"\"\"Attention-based pooling that respects padding.\"\"\"\n",
    "        # features: [batch_size, seq_len, hidden_dim]\n",
    "        # attention_mask: [batch_size, seq_len]\n",
    "        \n",
    "        masked_features = features * attention_mask.unsqueeze(-1)\n",
    "        pooled = masked_features.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddd149",
   "metadata": {},
   "source": [
    "### Gene-Aware Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c1b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneAwareContrastiveLoss(nn.Module):\n",
    "    \"\"\"Multi-level contrastive loss with gene family awareness.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.temperature = config.temperature\n",
    "        self.gene_weight = config.gene_contrastive_weight\n",
    "        self.isoform_weight = config.isoform_contrastive_weight\n",
    "        \n",
    "    def info_nce_loss(self, embeddings, labels, temperature):\n",
    "        \"\"\"Compute InfoNCE loss for contrastive learning.\"\"\"\n",
    "        batch_size = embeddings.shape[0]\n",
    "        \n",
    "        if batch_size < 2:\n",
    "            return torch.tensor(0.0, device=embeddings.device, requires_grad=True)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        sim_matrix = torch.matmul(embeddings, embeddings.T) / temperature\n",
    "        \n",
    "        # Create positive mask\n",
    "        labels = labels.view(-1, 1)\n",
    "        positive_mask = (labels == labels.T).float()\n",
    "        positive_mask.fill_diagonal_(0)  # Remove self-similarity\n",
    "        \n",
    "        # Check if there are any positive pairs\n",
    "        if positive_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=embeddings.device, requires_grad=True)\n",
    "        \n",
    "        # Numerically stable computation\n",
    "        # Subtract max for numerical stability\n",
    "        sim_matrix = sim_matrix - sim_matrix.max(dim=1, keepdim=True)[0].detach()\n",
    "        \n",
    "        exp_sim = torch.exp(sim_matrix)\n",
    "        \n",
    "        # Mask out diagonal (self-similarity)\n",
    "        mask = torch.eye(batch_size, device=embeddings.device, dtype=torch.bool)\n",
    "        exp_sim = exp_sim.masked_fill(mask, 0)\n",
    "        \n",
    "        # Compute positive and negative terms\n",
    "        pos_sim = (exp_sim * positive_mask).sum(dim=1)\n",
    "        neg_sim = exp_sim.sum(dim=1)\n",
    "        \n",
    "        # Avoid log(0) by adding small epsilon\n",
    "        eps = 1e-8\n",
    "        loss = -torch.log((pos_sim + eps) / (neg_sim + eps))\n",
    "        \n",
    "        # Only compute loss for samples that have positive pairs\n",
    "        valid_samples = positive_mask.sum(dim=1) > 0\n",
    "        if valid_samples.sum() == 0:\n",
    "            return torch.tensor(0.0, device=embeddings.device, requires_grad=True)\n",
    "        \n",
    "        return loss[valid_samples].mean()\n",
    "    \n",
    "    def forward(self, gene_embeddings, isoform_embeddings, gene_labels, transcript_labels):\n",
    "        \"\"\"\n",
    "        Compute multi-level contrastive loss.\n",
    "        \n",
    "        Args:\n",
    "            gene_embeddings: [batch_size, gene_embedding_dim]\n",
    "            isoform_embeddings: [batch_size, embedding_dim]  \n",
    "            gene_labels: [batch_size] - gene identifiers\n",
    "            transcript_labels: [batch_size] - transcript identifiers\n",
    "        \"\"\"\n",
    "        \n",
    "        # Gene-level contrastive loss (easier task)\n",
    "        gene_loss = self.info_nce_loss(gene_embeddings, gene_labels, self.temperature)\n",
    "        \n",
    "        # Isoform-level contrastive loss (harder task)\n",
    "        isoform_loss = self.info_nce_loss(isoform_embeddings, transcript_labels, self.temperature * 0.5)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = self.gene_weight * gene_loss + self.isoform_weight * isoform_loss\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'gene_loss': gene_loss,\n",
    "            'isoform_loss': isoform_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717ed70",
   "metadata": {},
   "source": [
    "### Vector Store and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "386e872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneAwareVectorStore:\n",
    "    \"\"\"Vector store with gene family awareness for efficient search.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        self.config = config\n",
    "        self.gene_families = None\n",
    "        self.transcripts = None\n",
    "        \n",
    "        # Separate indices for gene and isoform embeddings\n",
    "        self.gene_index = None\n",
    "        self.isoform_index = None\n",
    "        \n",
    "        # Mappings\n",
    "        self.transcript_to_idx = {}\n",
    "        self.idx_to_transcript = {}\n",
    "        self.transcript_to_gene = {}\n",
    "        \n",
    "        # Embeddings storage\n",
    "        self.gene_embeddings = None\n",
    "        self.isoform_embeddings = None\n",
    "        self.gene_uncertainties = None\n",
    "        self.isoform_uncertainties = None\n",
    "        \n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def build_index(self, \n",
    "                   gene_families: Dict[str, GeneFamilyInfo],\n",
    "                   transcripts: Dict[str, TranscriptInfo],\n",
    "                   model: nn.Module,\n",
    "                   tokenizer):\n",
    "        \"\"\"Build gene-aware search indices.\"\"\"\n",
    "        \n",
    "        self.gene_families = gene_families\n",
    "        self.transcripts = transcripts\n",
    "        \n",
    "        self.logger.info(\"Building gene-aware vector indices...\")\n",
    "        \n",
    "        # Prepare transcript data\n",
    "        transcript_ids = list(transcripts.keys())\n",
    "        transcript_sequences = [transcripts[tid].sequence for tid in transcript_ids]\n",
    "        \n",
    "        # Create mappings\n",
    "        for idx, tid in enumerate(transcript_ids):\n",
    "            self.transcript_to_idx[tid] = idx\n",
    "            self.idx_to_transcript[idx] = tid\n",
    "            self.transcript_to_gene[tid] = transcripts[tid].gene_name\n",
    "        \n",
    "        # Generate embeddings in batches\n",
    "        model.eval()\n",
    "        gene_embeddings_list = []\n",
    "        isoform_embeddings_list = []\n",
    "        gene_uncertainties_list = []\n",
    "        isoform_uncertainties_list = []\n",
    "        \n",
    "        batch_size = self.config.batch_size\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(transcript_sequences), batch_size):\n",
    "                batch_seqs = transcript_sequences[i:i+batch_size]\n",
    "                \n",
    "                # Tokenize batch\n",
    "                batch_tokens = []\n",
    "                for seq in batch_seqs:\n",
    "                    tokens = tokenizer(\n",
    "                        seq.upper().replace('N', 'A'),\n",
    "                        max_length=self.config.max_sequence_length,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    batch_tokens.append(tokens)\n",
    "                \n",
    "                # Stack tensors\n",
    "                input_ids = torch.stack([t['input_ids'].squeeze(0) for t in batch_tokens]).to(self.config.device)\n",
    "                attention_mask = torch.stack([t['attention_mask'].squeeze(0) for t in batch_tokens]).to(self.config.device)\n",
    "                \n",
    "                # Get embeddings\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                \n",
    "                gene_embeddings_list.append(outputs['gene_embedding'].cpu().numpy())\n",
    "                isoform_embeddings_list.append(outputs['isoform_embedding'].cpu().numpy())\n",
    "                gene_uncertainties_list.append(outputs['gene_uncertainty'].cpu().numpy())\n",
    "                isoform_uncertainties_list.append(outputs['isoform_uncertainty'].cpu().numpy())\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        self.gene_embeddings = np.vstack(gene_embeddings_list).astype('float32')\n",
    "        self.isoform_embeddings = np.vstack(isoform_embeddings_list).astype('float32')\n",
    "        self.gene_uncertainties = np.vstack(gene_uncertainties_list).astype('float32')\n",
    "        self.isoform_uncertainties = np.vstack(isoform_uncertainties_list).astype('float32')\n",
    "        \n",
    "        # Build FAISS indices\n",
    "        gene_dim = self.gene_embeddings.shape[1]\n",
    "        isoform_dim = self.isoform_embeddings.shape[1]\n",
    "        \n",
    "        self.gene_index = faiss.IndexFlatIP(gene_dim)\n",
    "        self.isoform_index = faiss.IndexFlatIP(isoform_dim)\n",
    "        \n",
    "        self.gene_index.add(self.gene_embeddings)\n",
    "        self.isoform_index.add(self.isoform_embeddings)\n",
    "        \n",
    "        self.logger.info(f\"Built indices with {len(transcript_ids)} transcripts\")\n",
    "        self.logger.info(f\"Gene embedding dim: {gene_dim}, Isoform embedding dim: {isoform_dim}\")\n",
    "    \n",
    "    def hierarchical_search(self, \n",
    "                          gene_embedding: np.ndarray,\n",
    "                          isoform_embedding: np.ndarray,\n",
    "                          top_k_genes: int = 10,\n",
    "                          top_k_isoforms: int = 5) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Hierarchical search: first find candidate genes, then best isoforms within those genes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Gene-level search\n",
    "        gene_similarities, gene_indices = self.gene_index.search(\n",
    "            gene_embedding.astype('float32'), top_k_genes\n",
    "        )\n",
    "        \n",
    "        # Get candidate genes\n",
    "        candidate_transcripts = []\n",
    "        candidate_genes = set()\n",
    "        \n",
    "        for idx in gene_indices[0]:\n",
    "            transcript_id = self.idx_to_transcript[idx]\n",
    "            gene_name = self.transcript_to_gene[transcript_id]\n",
    "            candidate_genes.add(gene_name)\n",
    "        \n",
    "        # Step 2: Collect all transcripts from candidate genes\n",
    "        for gene_name in candidate_genes:\n",
    "            gene_family = self.gene_families[gene_name]\n",
    "            candidate_transcripts.extend(gene_family.transcript_ids)\n",
    "        \n",
    "        # Step 3: Isoform-level search within candidates\n",
    "        if candidate_transcripts:\n",
    "            candidate_indices = [self.transcript_to_idx[tid] for tid in candidate_transcripts]\n",
    "            candidate_embeddings = self.isoform_embeddings[candidate_indices]\n",
    "            \n",
    "            # Compute similarities\n",
    "            isoform_similarities = np.dot(candidate_embeddings, isoform_embedding.T).flatten()\n",
    "            \n",
    "            # Get top isoforms\n",
    "            top_indices = np.argsort(isoform_similarities)[-top_k_isoforms:][::-1]\n",
    "            \n",
    "            results = []\n",
    "            for i, idx in enumerate(top_indices):\n",
    "                global_idx = candidate_indices[idx]\n",
    "                transcript_id = self.idx_to_transcript[global_idx]\n",
    "                gene_name = self.transcript_to_gene[transcript_id]\n",
    "                \n",
    "                results.append({\n",
    "                    'transcript_id': transcript_id,\n",
    "                    'gene_name': gene_name,\n",
    "                    'gene_similarity': float(gene_similarities[0][0]),  # Use first gene match\n",
    "                    'isoform_similarity': float(isoform_similarities[idx]),\n",
    "                    'gene_uncertainty': float(self.gene_uncertainties[global_idx][0]),\n",
    "                    'isoform_uncertainty': float(self.isoform_uncertainties[global_idx][0]),\n",
    "                    'global_idx': global_idx\n",
    "                })\n",
    "        else:\n",
    "            results = []\n",
    "        \n",
    "        return {\n",
    "            'results': results,\n",
    "            'candidate_genes': list(candidate_genes),\n",
    "            'num_candidates': len(candidate_transcripts)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71758c",
   "metadata": {},
   "source": [
    "### Uncertainty-Aware Assignment Engine  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b241647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncertaintyAwareAssignment:\n",
    "    \"\"\"Advanced assignment engine with multiple strategies and uncertainty quantification.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Assignment thresholds (will be learned/adapted)\n",
    "        self.gene_threshold = config.gene_similarity_threshold\n",
    "        self.isoform_threshold = config.isoform_similarity_threshold\n",
    "        self.uncertainty_threshold = config.uncertainty_threshold\n",
    "        \n",
    "    def compute_assignment_confidence(self, \n",
    "                                    gene_similarity: float,\n",
    "                                    isoform_similarity: float,\n",
    "                                    gene_uncertainty: float,\n",
    "                                    isoform_uncertainty: float,\n",
    "                                    num_candidates: int) -> float:\n",
    "        \"\"\"Compute overall assignment confidence score.\"\"\"\n",
    "        \n",
    "        # Similarity component (higher = better)\n",
    "        sim_score = 0.4 * gene_similarity + 0.6 * isoform_similarity\n",
    "        \n",
    "        # Uncertainty component (lower = better, so we take 1 - uncertainty)\n",
    "        uncertainty_score = 0.4 * (1 - gene_uncertainty) + 0.6 * (1 - isoform_uncertainty)\n",
    "        \n",
    "        # Complexity penalty (more candidates = lower confidence)\n",
    "        complexity_penalty = 1.0 / np.log(num_candidates + 2)\n",
    "        \n",
    "        # Combined confidence\n",
    "        confidence = sim_score * uncertainty_score * complexity_penalty\n",
    "        \n",
    "        return np.clip(confidence, 0.0, 1.0)\n",
    "    \n",
    "    def adaptive_threshold_selection(self,\n",
    "                                   similarities: List[float],\n",
    "                                   uncertainties: List[float],\n",
    "                                   gene_complexity: int) -> float:\n",
    "        \"\"\"Adaptively select similarity threshold based on context.\"\"\"\n",
    "        \n",
    "        if not similarities:\n",
    "            return self.isoform_threshold\n",
    "        \n",
    "        # Base threshold\n",
    "        base_threshold = self.isoform_threshold\n",
    "        \n",
    "        # Adjust based on gene complexity\n",
    "        complexity_adjustment = min(0.1, gene_complexity * 0.02)\n",
    "        \n",
    "        # Adjust based on uncertainty distribution\n",
    "        mean_uncertainty = np.mean(uncertainties)\n",
    "        uncertainty_adjustment = mean_uncertainty * 0.1\n",
    "        \n",
    "        # Adjust based on similarity gap\n",
    "        if len(similarities) > 1:\n",
    "            top_sim = max(similarities)\n",
    "            second_sim = sorted(similarities, reverse=True)[1]\n",
    "            gap = top_sim - second_sim\n",
    "            gap_adjustment = -gap * 0.05  # Lower threshold if there's a clear winner\n",
    "        else:\n",
    "            gap_adjustment = 0.0\n",
    "        \n",
    "        adaptive_threshold = base_threshold + complexity_adjustment + uncertainty_adjustment + gap_adjustment\n",
    "        \n",
    "        return np.clip(adaptive_threshold, 0.3, 0.95)\n",
    "    \n",
    "    def hierarchical_assignment(self, search_results: Dict[str, any]) -> Dict[str, any]:\n",
    "        \"\"\"Hierarchical assignment strategy: gene first, then isoform.\"\"\"\n",
    "        \n",
    "        if not search_results['results']:\n",
    "            return {\n",
    "                'assigned_transcript': None,\n",
    "                'assigned_gene': None,\n",
    "                'confidence': 0.0,\n",
    "                'assignment_type': 'unassigned',\n",
    "                'reason': 'no_candidates'\n",
    "            }\n",
    "        \n",
    "        results = search_results['results']\n",
    "        \n",
    "        # Step 1: Gene-level filtering\n",
    "        gene_candidates = [r for r in results if r['gene_similarity'] >= self.gene_threshold]\n",
    "        \n",
    "        if not gene_candidates:\n",
    "            return {\n",
    "                'assigned_transcript': None,\n",
    "                'assigned_gene': None,\n",
    "                'confidence': 0.0,\n",
    "                'assignment_type': 'unassigned',\n",
    "                'reason': 'gene_threshold_not_met'\n",
    "            }\n",
    "        \n",
    "        # Step 2: Adaptive isoform threshold\n",
    "        isoform_sims = [r['isoform_similarity'] for r in gene_candidates]\n",
    "        isoform_uncs = [r['isoform_uncertainty'] for r in gene_candidates]\n",
    "        \n",
    "        adaptive_threshold = self.adaptive_threshold_selection(\n",
    "            isoform_sims, isoform_uncs, len(search_results['candidate_genes'])\n",
    "        )\n",
    "        \n",
    "        # Step 3: Isoform-level filtering\n",
    "        isoform_candidates = [r for r in gene_candidates \n",
    "                            if r['isoform_similarity'] >= adaptive_threshold]\n",
    "        \n",
    "        if not isoform_candidates:\n",
    "            return {\n",
    "                'assigned_transcript': None,\n",
    "                'assigned_gene': gene_candidates[0]['gene_name'],  # At least assign gene\n",
    "                'confidence': 0.0,\n",
    "                'assignment_type': 'gene_only',\n",
    "                'reason': 'isoform_threshold_not_met'\n",
    "            }\n",
    "        \n",
    "        # Step 4: Select best candidate\n",
    "        best_candidate = max(isoform_candidates, \n",
    "                           key=lambda x: self.compute_assignment_confidence(\n",
    "                               x['gene_similarity'], x['isoform_similarity'],\n",
    "                               x['gene_uncertainty'], x['isoform_uncertainty'],\n",
    "                               len(isoform_candidates)\n",
    "                           ))\n",
    "        \n",
    "        confidence = self.compute_assignment_confidence(\n",
    "            best_candidate['gene_similarity'], best_candidate['isoform_similarity'],\n",
    "            best_candidate['gene_uncertainty'], best_candidate['isoform_uncertainty'],\n",
    "            len(isoform_candidates)\n",
    "        )\n",
    "        \n",
    "        # Determine assignment type\n",
    "        if confidence >= self.uncertainty_threshold:\n",
    "            assignment_type = 'high_confidence'\n",
    "        elif len(isoform_candidates) == 1:\n",
    "            assignment_type = 'unique_match'\n",
    "        else:\n",
    "            assignment_type = 'best_match'\n",
    "        \n",
    "        return {\n",
    "            'assigned_transcript': best_candidate['transcript_id'],\n",
    "            'assigned_gene': best_candidate['gene_name'],\n",
    "            'confidence': confidence,\n",
    "            'assignment_type': assignment_type,\n",
    "            'reason': 'successful_assignment',\n",
    "            'num_gene_candidates': len(gene_candidates),\n",
    "            'num_isoform_candidates': len(isoform_candidates),\n",
    "            'adaptive_threshold': adaptive_threshold,\n",
    "            'all_candidates': isoform_candidates[:3]  # Keep top 3 for analysis\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4cdbe",
   "metadata": {},
   "source": [
    "### DeepQuantV2 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8562f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQuantV2:\n",
    "    \"\"\"\n",
    "    Main DeepQuant V2 class that orchestrates the entire pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.device)\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=getattr(logging, config.log_level),\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.parser = TranscriptomeParser(config)\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.vector_store = GeneAwareVectorStore(config)\n",
    "        self.assignment_engine = UncertaintyAwareAssignment(config)\n",
    "        \n",
    "        # Data storage\n",
    "        self.gene_families = None\n",
    "        self.transcripts = None\n",
    "        self.training_stats = {}\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    def load_data(self, transcriptome_path: str, reads_path: str = None, ground_truth_path: str = None):\n",
    "        \"\"\"Load transcriptome, reads, and ground truth data.\"\"\"\n",
    "        \n",
    "        # Load transcriptome\n",
    "        self.gene_families, self.transcripts = self.parser.load_transcriptome(transcriptome_path)\n",
    "        \n",
    "        # Analyze gene families\n",
    "        self.gene_family_stats = self.parser.analyze_gene_families(self.gene_families)\n",
    "        \n",
    "        # Load reads if provided\n",
    "        self.reads = None\n",
    "        if reads_path:\n",
    "            self.reads = self._load_reads(reads_path)\n",
    "        \n",
    "        # Load ground truth if provided\n",
    "        self.ground_truth = None\n",
    "        if ground_truth_path:\n",
    "            self.ground_truth = pd.read_csv(ground_truth_path)\n",
    "            self.logger.info(f\"Loaded ground truth with {len(self.ground_truth)} entries\")\n",
    "    \n",
    "    def _load_reads(self, reads_path: str) -> List[str]:\n",
    "        \"\"\"Load sequencing reads from FASTQ file.\"\"\"\n",
    "        self.logger.info(f\"Loading reads from {reads_path}\")\n",
    "        \n",
    "        reads = []\n",
    "        for record in SeqIO.parse(reads_path, \"fastq\"):\n",
    "            reads.append(str(record.seq))\n",
    "        \n",
    "        self.logger.info(f\"Loaded {len(reads)} reads\")\n",
    "        return reads\n",
    "    \n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initialize the neural model and tokenizer.\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Initializing model...\")\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = GeneHierarchicalEncoder(self.config).to(self.device)\n",
    "        \n",
    "        # Initialize loss function\n",
    "        self.contrastive_loss = GeneAwareContrastiveLoss(self.config)\n",
    "        \n",
    "        self.logger.info(\"Model initialized successfully\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the model with gene-aware contrastive learning.\"\"\"\n",
    "        \n",
    "        if not self.model:\n",
    "            self.initialize_model()\n",
    "        \n",
    "        if not self.ground_truth is not None:\n",
    "            raise ValueError(\"Ground truth data required for training\")\n",
    "        \n",
    "        self.logger.info(\"Starting training...\")\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        train_dataset = GeneAwareDataset(\n",
    "            reads=self.reads,\n",
    "            ground_truth=self.ground_truth,\n",
    "            gene_families=self.gene_families,\n",
    "            transcripts=self.transcripts,\n",
    "            config=self.config,\n",
    "            mode=\"train\"\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.config.num_workers,\n",
    "            collate_fn=custom_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Initialize optimizer and scheduler\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay\n",
    "        )\n",
    "        \n",
    "        total_steps = len(train_loader) * self.config.num_epochs\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.config.learning_rate,\n",
    "            total_steps=total_steps,\n",
    "            pct_start=0.1\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        training_losses = []\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            epoch_losses = []\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                # Move to device\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                \n",
    "                # Skip batch if no training labels or gene names\n",
    "                if 'true_transcript' not in batch or 'gene_name' not in batch:\n",
    "                    continue\n",
    "                \n",
    "                # Filter out None values in gene_name and true_transcript\n",
    "                valid_indices = []\n",
    "                valid_gene_names = []\n",
    "                valid_transcripts = []\n",
    "                \n",
    "                for i, (gene_name, transcript_id) in enumerate(zip(batch['gene_name'], batch['true_transcript'])):\n",
    "                    if gene_name is not None and transcript_id is not None:\n",
    "                        valid_indices.append(i)\n",
    "                        valid_gene_names.append(gene_name)\n",
    "                        valid_transcripts.append(transcript_id)\n",
    "                \n",
    "                if not valid_indices:\n",
    "                    continue  # Skip batch if no valid samples\n",
    "                \n",
    "                # Filter tensors to valid indices only\n",
    "                input_ids = input_ids[valid_indices]\n",
    "                attention_mask = attention_mask[valid_indices]\n",
    "                \n",
    "                # Create gene and transcript labels for contrastive learning\n",
    "                gene_labels = []\n",
    "                transcript_labels = []\n",
    "                \n",
    "                for gene_name, transcript_id in zip(valid_gene_names, valid_transcripts):\n",
    "                    # Convert to numeric labels (hash for consistency)\n",
    "                    gene_labels.append(hash(gene_name) % 10000)\n",
    "                    transcript_labels.append(hash(transcript_id) % 100000)\n",
    "                \n",
    "                if not gene_labels:  # Skip if no valid labels\n",
    "                    continue\n",
    "                    \n",
    "                gene_labels = torch.tensor(gene_labels, device=self.device)\n",
    "                transcript_labels = torch.tensor(transcript_labels, device=self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                \n",
    "                # Compute contrastive loss\n",
    "                loss_dict = self.contrastive_loss(\n",
    "                    outputs['gene_embedding'],\n",
    "                    outputs['isoform_embedding'],\n",
    "                    gene_labels,\n",
    "                    transcript_labels\n",
    "                )\n",
    "                \n",
    "                total_loss = loss_dict['total_loss']\n",
    "\n",
    "                # Check for invalid loss values\n",
    "                if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
    "                    self.logger.warning(f\"Invalid loss detected: {total_loss.item()}, skipping batch\")\n",
    "                    continue\n",
    "\n",
    "                # Skip if loss is too large (potential exploding gradient)\n",
    "                if total_loss.item() > 100:\n",
    "                    self.logger.warning(f\"Loss too large: {total_loss.item()}, skipping batch\")\n",
    "                    continue\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "\n",
    "                # Add gradient clipping to prevent exploding gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Record loss\n",
    "                epoch_losses.append(total_loss.item())\n",
    "                \n",
    "                # Log progress\n",
    "                if batch_idx % 50 == 0:\n",
    "                    self.logger.info(\n",
    "                        f\"Epoch {epoch+1}/{self.config.num_epochs}, \"\n",
    "                        f\"Batch {batch_idx}/{len(train_loader)}, \"\n",
    "                        f\"Loss: {total_loss.item():.4f}\"\n",
    "                    )\n",
    "            \n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "            training_losses.append(avg_loss)\n",
    "            \n",
    "            self.logger.info(f\"Epoch {epoch+1} completed. Average loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save training stats\n",
    "        self.training_stats = {\n",
    "            'training_losses': training_losses,\n",
    "            'final_loss': training_losses[-1] if training_losses else 0,\n",
    "            'num_epochs': self.config.num_epochs,\n",
    "            'total_batches': len(train_loader) * self.config.num_epochs\n",
    "        }\n",
    "        \n",
    "        self.logger.info(\"Training completed!\")\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build the gene-aware vector search index.\"\"\"\n",
    "        \n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model must be initialized before building index\")\n",
    "        \n",
    "        self.vector_store.build_index(\n",
    "            self.gene_families,\n",
    "            self.transcripts,\n",
    "            self.model,\n",
    "            self.tokenizer\n",
    "        )\n",
    "    \n",
    "    def quantify_reads(self, \n",
    "                      reads: List[str] = None,\n",
    "                      top_k_genes: int = 10,\n",
    "                      top_k_isoforms: int = 5) -> Dict[str, any]:\n",
    "        \"\"\"Quantify reads using gene-aware hierarchical assignment.\"\"\"\n",
    "        \n",
    "        if reads is None:\n",
    "            reads = self.reads\n",
    "        \n",
    "        if not reads:\n",
    "            raise ValueError(\"No reads provided for quantification\")\n",
    "        \n",
    "        if self.vector_store.gene_index is None:\n",
    "            raise ValueError(\"Vector store index must be built before quantification\")\n",
    "        \n",
    "        self.logger.info(f\"Quantifying {len(reads)} reads...\")\n",
    "        \n",
    "        # Results storage\n",
    "        assignments = []\n",
    "        assignment_stats = {\n",
    "            'total_reads': len(reads),\n",
    "            'high_confidence': 0,\n",
    "            'unique_match': 0,\n",
    "            'best_match': 0,\n",
    "            'gene_only': 0,\n",
    "            'unassigned': 0\n",
    "        }\n",
    "        \n",
    "        # Process reads in batches\n",
    "        batch_size = self.config.batch_size\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(reads), batch_size):\n",
    "                batch_reads = reads[i:i+batch_size]\n",
    "                \n",
    "                # Tokenize batch\n",
    "                batch_tokens = []\n",
    "                for read in batch_reads:\n",
    "                    tokens = self.tokenizer(\n",
    "                        read.upper().replace('N', 'A'),\n",
    "                        max_length=self.config.max_sequence_length,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    batch_tokens.append(tokens)\n",
    "                \n",
    "                # Stack tensors\n",
    "                input_ids = torch.stack([t['input_ids'].squeeze(0) for t in batch_tokens]).to(self.device)\n",
    "                attention_mask = torch.stack([t['attention_mask'].squeeze(0) for t in batch_tokens]).to(self.device)\n",
    "                \n",
    "                # Get embeddings\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                \n",
    "                # Process each read in the batch\n",
    "                for j, read in enumerate(batch_reads):\n",
    "                    read_idx = i + j\n",
    "                    \n",
    "                    # Extract embeddings for this read\n",
    "                    gene_emb = outputs['gene_embedding'][j:j+1].cpu().numpy()\n",
    "                    isoform_emb = outputs['isoform_embedding'][j:j+1].cpu().numpy()\n",
    "                    \n",
    "                    # Hierarchical search\n",
    "                    search_results = self.vector_store.hierarchical_search(\n",
    "                        gene_emb, isoform_emb, top_k_genes, top_k_isoforms\n",
    "                    )\n",
    "                    \n",
    "                    # Assignment\n",
    "                    assignment = self.assignment_engine.hierarchical_assignment(search_results)\n",
    "                    \n",
    "                    # Record assignment\n",
    "                    assignment['read_id'] = f\"read_{read_idx:06d}\"\n",
    "                    assignment['read_sequence'] = read\n",
    "                    assignments.append(assignment)\n",
    "                    \n",
    "                    # Update stats\n",
    "                    assignment_stats[assignment['assignment_type']] += 1\n",
    "                \n",
    "                # Log progress\n",
    "                if i % (batch_size * 10) == 0:\n",
    "                    self.logger.info(f\"Processed {i+len(batch_reads)}/{len(reads)} reads\")\n",
    "        \n",
    "        self.logger.info(\"Quantification completed!\")\n",
    "        self.logger.info(f\"Assignment statistics: {assignment_stats}\")\n",
    "        \n",
    "        return {\n",
    "            'assignments': assignments,\n",
    "            'stats': assignment_stats,\n",
    "            'transcript_counts': self._compute_transcript_counts(assignments)\n",
    "        }\n",
    "    \n",
    "    def _compute_transcript_counts(self, assignments: List[Dict]) -> Dict[str, Dict]:\n",
    "        \"\"\"Compute transcript abundance estimates from assignments.\"\"\"\n",
    "        \n",
    "        transcript_counts = defaultdict(lambda: {\n",
    "            'raw_count': 0,\n",
    "            'weighted_count': 0.0,\n",
    "            'confidence_sum': 0.0,\n",
    "            'assignments': []\n",
    "        })\n",
    "        \n",
    "        for assignment in assignments:\n",
    "            if assignment['assigned_transcript']:\n",
    "                tid = assignment['assigned_transcript']\n",
    "                confidence = assignment['confidence']\n",
    "                \n",
    "                transcript_counts[tid]['raw_count'] += 1\n",
    "                transcript_counts[tid]['weighted_count'] += confidence\n",
    "                transcript_counts[tid]['confidence_sum'] += confidence\n",
    "                transcript_counts[tid]['assignments'].append(assignment)\n",
    "        \n",
    "        # Compute final metrics\n",
    "        final_counts = {}\n",
    "        total_weighted = sum(tc['weighted_count'] for tc in transcript_counts.values())\n",
    "        \n",
    "        for tid, counts in transcript_counts.items():\n",
    "            num_assignments = len(counts['assignments'])\n",
    "            avg_confidence = counts['confidence_sum'] / num_assignments if num_assignments > 0 else 0.0\n",
    "            \n",
    "            final_counts[tid] = {\n",
    "                'raw_count': counts['raw_count'],\n",
    "                'weighted_count': counts['weighted_count'],\n",
    "                'tpm': (counts['weighted_count'] / total_weighted * 1e6) if total_weighted > 0 else 0.0,\n",
    "                'average_confidence': avg_confidence,\n",
    "                'gene_name': self.transcripts[tid].gene_name if tid in self.transcripts else 'unknown'\n",
    "            }\n",
    "        \n",
    "        return final_counts\n",
    "    \n",
    "    def save_results(self, results: Dict[str, any], output_prefix: str = \"deepquant_v2\"):\n",
    "        \"\"\"Save quantification results to files.\"\"\"\n",
    "        \n",
    "        output_dir = Path(self.config.output_dir)\n",
    "        \n",
    "        # Save assignments\n",
    "        assignments_df = pd.DataFrame(results['assignments'])\n",
    "        assignments_file = output_dir / f\"{output_prefix}_assignments.csv\"\n",
    "        assignments_df.to_csv(assignments_file, index=False)\n",
    "        \n",
    "        # Save transcript counts\n",
    "        counts_data = []\n",
    "        for tid, counts in results['transcript_counts'].items():\n",
    "            counts_data.append({\n",
    "                'transcript_id': tid,\n",
    "                'gene_name': counts['gene_name'],\n",
    "                'raw_count': counts['raw_count'],\n",
    "                'weighted_count': counts['weighted_count'],\n",
    "                'tpm': counts['tpm'],\n",
    "                'average_confidence': counts['average_confidence']\n",
    "            })\n",
    "        \n",
    "        counts_df = pd.DataFrame(counts_data).sort_values('weighted_count', ascending=False)\n",
    "        counts_file = output_dir / f\"{output_prefix}_transcript_counts.csv\"\n",
    "        counts_df.to_csv(counts_file, index=False)\n",
    "        \n",
    "        # Save statistics\n",
    "        stats_file = output_dir / f\"{output_prefix}_stats.json\"\n",
    "        with open(stats_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'assignment_stats': results['stats'],\n",
    "                'gene_family_stats': self.gene_family_stats,\n",
    "                'training_stats': self.training_stats,\n",
    "                'config': self.config.__dict__\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Results saved to {output_dir}\")\n",
    "        self.logger.info(f\"  Assignments: {assignments_file}\")\n",
    "        self.logger.info(f\"  Counts: {counts_file}\")\n",
    "        self.logger.info(f\"  Stats: {stats_file}\")\n",
    "        \n",
    "        return {\n",
    "            'assignments_file': assignments_file,\n",
    "            'counts_file': counts_file,\n",
    "            'stats_file': stats_file\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6cf0ec",
   "metadata": {},
   "source": [
    "### Evaluation and Validation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da404799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQuantEvaluator:\n",
    "    \"\"\"Comprehensive evaluation framework for DeepQuant V2.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DeepQuantConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def evaluate_against_ground_truth(self,\n",
    "                                    assignments: List[Dict],\n",
    "                                    ground_truth: pd.DataFrame) -> Dict[str, any]:\n",
    "        \"\"\"Evaluate assignments against ground truth.\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Evaluating against ground truth...\")\n",
    "        \n",
    "        # Align assignments with ground truth\n",
    "        eval_data = []\n",
    "        \n",
    "        for i, assignment in enumerate(assignments):\n",
    "            if i < len(ground_truth):\n",
    "                gt_row = ground_truth.iloc[i]\n",
    "                \n",
    "                eval_data.append({\n",
    "                    'read_id': assignment['read_id'],\n",
    "                    'predicted_transcript': assignment.get('assigned_transcript'),\n",
    "                    'predicted_gene': assignment.get('assigned_gene'), \n",
    "                    'true_transcript': gt_row['true_transcript'],\n",
    "                    'true_gene': gt_row.get('true_gene', 'unknown'),\n",
    "                    'confidence': assignment.get('confidence', 0.0),\n",
    "                    'assignment_type': assignment.get('assignment_type', 'unknown')\n",
    "                })\n",
    "        \n",
    "        eval_df = pd.DataFrame(eval_data)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = self._compute_evaluation_metrics(eval_df)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _compute_evaluation_metrics(self, eval_df: pd.DataFrame) -> Dict[str, any]:\n",
    "        \"\"\"Compute comprehensive evaluation metrics.\"\"\"\n",
    "        \n",
    "        total_reads = len(eval_df)\n",
    "        \n",
    "        # Basic assignment metrics\n",
    "        assigned_reads = eval_df['predicted_transcript'].notna().sum()\n",
    "        assignment_rate = assigned_reads / total_reads\n",
    "        \n",
    "        # Transcript-level accuracy\n",
    "        transcript_correct = (eval_df['predicted_transcript'] == eval_df['true_transcript']).sum()\n",
    "        transcript_accuracy = transcript_correct / total_reads\n",
    "        \n",
    "        assigned_df = eval_df[eval_df['predicted_transcript'].notna()].copy()\n",
    "        if len(assigned_df) > 0:\n",
    "            weighted_transcript_acc = ((assigned_df['predicted_transcript'] == assigned_df['true_transcript']) * \n",
    "                                     assigned_df['confidence']).sum() / assigned_df['confidence'].sum()\n",
    "            avg_confidence = assigned_df['confidence'].mean()\n",
    "        else:\n",
    "            weighted_transcript_acc = 0.0\n",
    "        # Gene-level accuracy (including partial assignments)\n",
    "        gene_correct = (eval_df['predicted_gene'] == eval_df['true_gene']).sum()\n",
    "        gene_accuracy = gene_correct / total_reads\n",
    "        \n",
    "        # Confidence-weighted metrics\n",
    "        assigned_df = eval_df[eval_df['predicted_transcript'].notna()].copy()\n",
    "        if len(assigned_df) > 0:\n",
    "            weighted_transcript_acc = ((assigned_df['predicted_transcript'] == assigned_df['true_transcript']) * \n",
    "                                     assigned_df['confidence']).sum() / assigned_df['confidence'].sum()\n",
    "            avg_confidence = assigned_df['confidence'].mean()\n",
    "        else:\n",
    "            weighted_transcript_acc = 0.0\n",
    "            avg_confidence = 0.0\n",
    "        \n",
    "        # Assignment type breakdown\n",
    "        assignment_breakdown = eval_df['assignment_type'].value_counts().to_dict()\n",
    "        \n",
    "        # Confidence analysis\n",
    "        confidence_stats = {\n",
    "            'mean': float(eval_df['confidence'].mean()),\n",
    "            'std': float(eval_df['confidence'].std()),\n",
    "            'median': float(eval_df['confidence'].median()),\n",
    "            'q75': float(eval_df['confidence'].quantile(0.75)),\n",
    "            'q25': float(eval_df['confidence'].quantile(0.25))\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'total_reads': total_reads,\n",
    "            'assignment_rate': assignment_rate,\n",
    "            'transcript_accuracy': transcript_accuracy,\n",
    "            'gene_accuracy': gene_accuracy,\n",
    "            'weighted_transcript_accuracy': weighted_transcript_acc,\n",
    "            'average_confidence': avg_confidence,\n",
    "            'assignment_breakdown': assignment_breakdown,\n",
    "            'confidence_stats': confidence_stats\n",
    "        }\n",
    "    \n",
    "    def generate_evaluation_report(self, metrics: Dict[str, any], output_dir: str):\n",
    "        \"\"\"Generate comprehensive evaluation report with visualizations.\"\"\"\n",
    "        \n",
    "        output_path = Path(output_dir)\n",
    "        \n",
    "        # Create evaluation report\n",
    "        report = {\n",
    "            'summary': {\n",
    "                'assignment_rate': f\"{metrics['assignment_rate']:.1%}\",\n",
    "                'transcript_accuracy': f\"{metrics['transcript_accuracy']:.1%}\",\n",
    "                'gene_accuracy': f\"{metrics['gene_accuracy']:.1%}\",\n",
    "                'average_confidence': f\"{metrics['average_confidence']:.3f}\"\n",
    "            },\n",
    "            'detailed_metrics': metrics\n",
    "        }\n",
    "        \n",
    "        # Save report\n",
    "        report_file = output_path / \"evaluation_report.json\"\n",
    "        with open(report_file, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"Evaluation report saved to {report_file}\")\n",
    "        \n",
    "        return report_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755750da",
   "metadata": {},
   "source": [
    "### Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc57725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 20:13:26,058 - __main__ - INFO - Loading transcriptome from /run/media/saadat/A/tools/DeepQuant/gencode.v47.transcripts_500.fa\n",
      "2025-08-25 20:13:26,067 - __main__ - INFO - Loaded 500 transcripts, 25 gene families\n",
      "2025-08-25 20:13:26,067 - __main__ - INFO - Skipped 0 transcripts\n",
      "2025-08-25 20:13:26,068 - __main__ - INFO - Analyzing gene family complexity...\n",
      "2025-08-25 20:13:26,068 - __main__ - INFO - Gene family analysis complete:\n",
      "2025-08-25 20:13:26,068 - __main__ - INFO -   Total genes: 25\n",
      "2025-08-25 20:13:26,068 - __main__ - INFO -   Single isoform genes: 16\n",
      "2025-08-25 20:13:26,068 - __main__ - INFO -   Multi-isoform genes: 9\n",
      "2025-08-25 20:13:26,069 - __main__ - INFO -   Most complex gene: 286 isoforms\n",
      "2025-08-25 20:13:26,069 - __main__ - INFO - Loading reads from /run/media/saadat/A/tools/DeepQuant/simulated_reads.fastq\n",
      "2025-08-25 20:13:26,097 - __main__ - INFO - Loaded 5000 reads\n",
      "2025-08-25 20:13:26,111 - __main__ - INFO - Loaded ground truth with 5000 entries\n",
      "2025-08-25 20:13:26,112 - __main__ - INFO - Initializing model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧬 Loading transcriptome and data...\n",
      "🤖 Initializing and training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 20:13:31.064126: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-25 20:13:31.156848: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756145611.219347    5784 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756145611.234222    5784 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756145611.311833    5784 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756145611.311850    5784 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756145611.311851    5784 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756145611.311852    5784 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-25 20:13:31.315983: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-25 20:13:34,289 - __main__ - INFO - Model initialized successfully\n",
      "2025-08-25 20:13:34,289 - __main__ - INFO - Starting training...\n",
      "2025-08-25 20:13:36,061 - __main__ - INFO - Epoch 1/5, Batch 0/625, Loss: 0.1522\n",
      "2025-08-25 20:14:02,257 - __main__ - INFO - Epoch 1/5, Batch 50/625, Loss: 0.2832\n",
      "2025-08-25 20:14:27,844 - __main__ - INFO - Epoch 1/5, Batch 100/625, Loss: 0.5055\n",
      "2025-08-25 20:14:53,347 - __main__ - INFO - Epoch 1/5, Batch 150/625, Loss: 0.1253\n",
      "2025-08-25 20:15:19,224 - __main__ - INFO - Epoch 1/5, Batch 200/625, Loss: 0.0580\n",
      "2025-08-25 20:15:45,256 - __main__ - INFO - Epoch 1/5, Batch 250/625, Loss: 0.2901\n",
      "2025-08-25 20:16:11,143 - __main__ - INFO - Epoch 1/5, Batch 300/625, Loss: 0.0963\n",
      "2025-08-25 20:16:37,059 - __main__ - INFO - Epoch 1/5, Batch 350/625, Loss: 0.4250\n",
      "2025-08-25 20:17:02,749 - __main__ - INFO - Epoch 1/5, Batch 400/625, Loss: 0.2710\n",
      "2025-08-25 20:17:28,337 - __main__ - INFO - Epoch 1/5, Batch 450/625, Loss: 0.1766\n",
      "2025-08-25 20:17:54,106 - __main__ - INFO - Epoch 1/5, Batch 500/625, Loss: 0.0073\n",
      "2025-08-25 20:18:19,785 - __main__ - INFO - Epoch 1/5, Batch 550/625, Loss: 0.4183\n",
      "2025-08-25 20:18:45,456 - __main__ - INFO - Epoch 1/5, Batch 600/625, Loss: 0.1577\n",
      "2025-08-25 20:18:57,895 - __main__ - INFO - Epoch 1 completed. Average loss: 0.2565\n",
      "2025-08-25 20:18:58,504 - __main__ - INFO - Epoch 2/5, Batch 0/625, Loss: 0.1042\n",
      "2025-08-25 20:19:24,829 - __main__ - INFO - Epoch 2/5, Batch 50/625, Loss: 0.9838\n",
      "2025-08-25 20:19:50,537 - __main__ - INFO - Epoch 2/5, Batch 100/625, Loss: 0.5970\n",
      "2025-08-25 20:20:16,191 - __main__ - INFO - Epoch 2/5, Batch 150/625, Loss: 0.0000\n",
      "2025-08-25 20:20:41,888 - __main__ - INFO - Epoch 2/5, Batch 200/625, Loss: 0.2312\n",
      "2025-08-25 20:21:07,640 - __main__ - INFO - Epoch 2/5, Batch 250/625, Loss: 0.0899\n",
      "2025-08-25 20:21:33,172 - __main__ - INFO - Epoch 2/5, Batch 300/625, Loss: 0.2391\n",
      "2025-08-25 20:21:58,950 - __main__ - INFO - Epoch 2/5, Batch 350/625, Loss: 0.0571\n",
      "2025-08-25 20:22:24,594 - __main__ - INFO - Epoch 2/5, Batch 400/625, Loss: 1.2426\n",
      "2025-08-25 20:22:50,388 - __main__ - INFO - Epoch 2/5, Batch 450/625, Loss: 0.7857\n",
      "2025-08-25 20:23:16,044 - __main__ - INFO - Epoch 2/5, Batch 500/625, Loss: 0.0864\n",
      "2025-08-25 20:23:42,019 - __main__ - INFO - Epoch 2/5, Batch 550/625, Loss: 0.0494\n",
      "2025-08-25 20:24:07,645 - __main__ - INFO - Epoch 2/5, Batch 600/625, Loss: 0.2654\n",
      "2025-08-25 20:24:20,182 - __main__ - INFO - Epoch 2 completed. Average loss: 0.2372\n",
      "2025-08-25 20:24:20,814 - __main__ - INFO - Epoch 3/5, Batch 0/625, Loss: 0.8641\n",
      "2025-08-25 20:24:46,541 - __main__ - INFO - Epoch 3/5, Batch 50/625, Loss: 0.1436\n",
      "2025-08-25 20:25:12,426 - __main__ - INFO - Epoch 3/5, Batch 100/625, Loss: 0.2074\n",
      "2025-08-25 20:25:38,287 - __main__ - INFO - Epoch 3/5, Batch 150/625, Loss: 0.0908\n",
      "2025-08-25 20:26:04,206 - __main__ - INFO - Epoch 3/5, Batch 200/625, Loss: 0.0710\n",
      "2025-08-25 20:26:29,843 - __main__ - INFO - Epoch 3/5, Batch 250/625, Loss: 0.0011\n",
      "2025-08-25 20:26:56,147 - __main__ - INFO - Epoch 3/5, Batch 300/625, Loss: 1.3622\n",
      "2025-08-25 20:27:21,918 - __main__ - INFO - Epoch 3/5, Batch 350/625, Loss: 0.0003\n",
      "2025-08-25 20:27:47,795 - __main__ - INFO - Epoch 3/5, Batch 400/625, Loss: 0.5257\n",
      "2025-08-25 20:28:13,556 - __main__ - INFO - Epoch 3/5, Batch 450/625, Loss: 0.0115\n",
      "2025-08-25 20:28:39,221 - __main__ - INFO - Epoch 3/5, Batch 500/625, Loss: 0.0807\n",
      "2025-08-25 20:29:05,081 - __main__ - INFO - Epoch 3/5, Batch 550/625, Loss: 0.0011\n",
      "2025-08-25 20:29:31,099 - __main__ - INFO - Epoch 3/5, Batch 600/625, Loss: 0.0002\n",
      "2025-08-25 20:29:43,661 - __main__ - INFO - Epoch 3 completed. Average loss: 0.2213\n",
      "2025-08-25 20:29:44,277 - __main__ - INFO - Epoch 4/5, Batch 0/625, Loss: 0.0106\n",
      "2025-08-25 20:30:10,415 - __main__ - INFO - Epoch 4/5, Batch 50/625, Loss: 0.0001\n",
      "2025-08-25 20:30:36,283 - __main__ - INFO - Epoch 4/5, Batch 100/625, Loss: 0.3388\n",
      "2025-08-25 20:31:02,048 - __main__ - INFO - Epoch 4/5, Batch 150/625, Loss: 0.0000\n",
      "2025-08-25 20:31:27,806 - __main__ - INFO - Epoch 4/5, Batch 200/625, Loss: 0.0004\n",
      "2025-08-25 20:31:53,593 - __main__ - INFO - Epoch 4/5, Batch 250/625, Loss: 0.1161\n",
      "2025-08-25 20:32:19,304 - __main__ - INFO - Epoch 4/5, Batch 300/625, Loss: 0.0467\n",
      "2025-08-25 20:32:45,168 - __main__ - INFO - Epoch 4/5, Batch 350/625, Loss: 0.2359\n",
      "2025-08-25 20:33:10,853 - __main__ - INFO - Epoch 4/5, Batch 400/625, Loss: 0.0602\n",
      "2025-08-25 20:33:36,611 - __main__ - INFO - Epoch 4/5, Batch 450/625, Loss: 0.2756\n",
      "2025-08-25 20:34:02,331 - __main__ - INFO - Epoch 4/5, Batch 500/625, Loss: 0.3152\n",
      "2025-08-25 20:34:28,121 - __main__ - INFO - Epoch 4/5, Batch 550/625, Loss: 0.0000\n",
      "2025-08-25 20:34:53,855 - __main__ - INFO - Epoch 4/5, Batch 600/625, Loss: 0.0001\n",
      "2025-08-25 20:35:06,150 - __main__ - INFO - Epoch 4 completed. Average loss: 0.1709\n",
      "2025-08-25 20:35:06,745 - __main__ - INFO - Epoch 5/5, Batch 0/625, Loss: 0.0002\n",
      "2025-08-25 20:35:32,485 - __main__ - INFO - Epoch 5/5, Batch 50/625, Loss: 0.1956\n",
      "2025-08-25 20:35:58,356 - __main__ - INFO - Epoch 5/5, Batch 100/625, Loss: 0.0000\n",
      "2025-08-25 20:36:24,226 - __main__ - INFO - Epoch 5/5, Batch 150/625, Loss: 0.0496\n",
      "2025-08-25 20:36:50,034 - __main__ - INFO - Epoch 5/5, Batch 200/625, Loss: 0.1739\n",
      "2025-08-25 20:37:15,759 - __main__ - INFO - Epoch 5/5, Batch 250/625, Loss: 0.2874\n",
      "2025-08-25 20:37:41,264 - __main__ - INFO - Epoch 5/5, Batch 300/625, Loss: 0.0134\n",
      "2025-08-25 20:38:05,158 - __main__ - INFO - Epoch 5/5, Batch 350/625, Loss: 0.0464\n",
      "2025-08-25 20:38:29,137 - __main__ - INFO - Epoch 5/5, Batch 400/625, Loss: 0.2056\n",
      "2025-08-25 20:38:54,042 - __main__ - INFO - Epoch 5/5, Batch 450/625, Loss: 0.9700\n",
      "2025-08-25 20:39:20,136 - __main__ - INFO - Epoch 5/5, Batch 500/625, Loss: 0.0533\n",
      "2025-08-25 20:39:45,754 - __main__ - INFO - Epoch 5/5, Batch 550/625, Loss: 0.0490\n",
      "2025-08-25 20:40:11,512 - __main__ - INFO - Epoch 5/5, Batch 600/625, Loss: 0.0000\n",
      "2025-08-25 20:40:23,933 - __main__ - INFO - Epoch 5 completed. Average loss: 0.1797\n",
      "2025-08-25 20:40:23,934 - __main__ - INFO - Training completed!\n",
      "2025-08-25 20:40:23,936 - __main__ - INFO - Building gene-aware vector indices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Building gene-aware search index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 20:40:33,540 - __main__ - INFO - Built indices with 500 transcripts\n",
      "2025-08-25 20:40:33,541 - __main__ - INFO - Gene embedding dim: 128, Isoform embedding dim: 256\n",
      "2025-08-25 20:40:33,541 - __main__ - INFO - Quantifying 5000 reads...\n",
      "2025-08-25 20:40:33,678 - __main__ - INFO - Processed 8/5000 reads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Quantifying reads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 20:40:35,031 - __main__ - INFO - Processed 88/5000 reads\n",
      "2025-08-25 20:40:36,387 - __main__ - INFO - Processed 168/5000 reads\n",
      "2025-08-25 20:40:37,742 - __main__ - INFO - Processed 248/5000 reads\n",
      "2025-08-25 20:40:39,102 - __main__ - INFO - Processed 328/5000 reads\n",
      "2025-08-25 20:40:40,459 - __main__ - INFO - Processed 408/5000 reads\n",
      "2025-08-25 20:40:41,822 - __main__ - INFO - Processed 488/5000 reads\n",
      "2025-08-25 20:40:43,184 - __main__ - INFO - Processed 568/5000 reads\n",
      "2025-08-25 20:40:44,543 - __main__ - INFO - Processed 648/5000 reads\n",
      "2025-08-25 20:40:45,906 - __main__ - INFO - Processed 728/5000 reads\n",
      "2025-08-25 20:40:47,263 - __main__ - INFO - Processed 808/5000 reads\n",
      "2025-08-25 20:40:48,625 - __main__ - INFO - Processed 888/5000 reads\n",
      "2025-08-25 20:40:49,984 - __main__ - INFO - Processed 968/5000 reads\n",
      "2025-08-25 20:40:51,341 - __main__ - INFO - Processed 1048/5000 reads\n",
      "2025-08-25 20:40:52,702 - __main__ - INFO - Processed 1128/5000 reads\n",
      "2025-08-25 20:40:54,061 - __main__ - INFO - Processed 1208/5000 reads\n",
      "2025-08-25 20:40:55,423 - __main__ - INFO - Processed 1288/5000 reads\n",
      "2025-08-25 20:40:56,784 - __main__ - INFO - Processed 1368/5000 reads\n",
      "2025-08-25 20:40:58,138 - __main__ - INFO - Processed 1448/5000 reads\n",
      "2025-08-25 20:40:59,498 - __main__ - INFO - Processed 1528/5000 reads\n",
      "2025-08-25 20:41:00,854 - __main__ - INFO - Processed 1608/5000 reads\n",
      "2025-08-25 20:41:02,218 - __main__ - INFO - Processed 1688/5000 reads\n",
      "2025-08-25 20:41:03,579 - __main__ - INFO - Processed 1768/5000 reads\n",
      "2025-08-25 20:41:04,934 - __main__ - INFO - Processed 1848/5000 reads\n",
      "2025-08-25 20:41:06,294 - __main__ - INFO - Processed 1928/5000 reads\n",
      "2025-08-25 20:41:07,655 - __main__ - INFO - Processed 2008/5000 reads\n",
      "2025-08-25 20:41:09,017 - __main__ - INFO - Processed 2088/5000 reads\n",
      "2025-08-25 20:41:10,376 - __main__ - INFO - Processed 2168/5000 reads\n",
      "2025-08-25 20:41:11,734 - __main__ - INFO - Processed 2248/5000 reads\n",
      "2025-08-25 20:41:13,094 - __main__ - INFO - Processed 2328/5000 reads\n",
      "2025-08-25 20:41:14,455 - __main__ - INFO - Processed 2408/5000 reads\n",
      "2025-08-25 20:41:15,817 - __main__ - INFO - Processed 2488/5000 reads\n",
      "2025-08-25 20:41:17,174 - __main__ - INFO - Processed 2568/5000 reads\n",
      "2025-08-25 20:41:18,532 - __main__ - INFO - Processed 2648/5000 reads\n",
      "2025-08-25 20:41:19,890 - __main__ - INFO - Processed 2728/5000 reads\n",
      "2025-08-25 20:41:21,247 - __main__ - INFO - Processed 2808/5000 reads\n",
      "2025-08-25 20:41:22,606 - __main__ - INFO - Processed 2888/5000 reads\n",
      "2025-08-25 20:41:23,967 - __main__ - INFO - Processed 2968/5000 reads\n",
      "2025-08-25 20:41:25,322 - __main__ - INFO - Processed 3048/5000 reads\n",
      "2025-08-25 20:41:26,681 - __main__ - INFO - Processed 3128/5000 reads\n",
      "2025-08-25 20:41:28,040 - __main__ - INFO - Processed 3208/5000 reads\n",
      "2025-08-25 20:41:29,395 - __main__ - INFO - Processed 3288/5000 reads\n",
      "2025-08-25 20:41:30,756 - __main__ - INFO - Processed 3368/5000 reads\n",
      "2025-08-25 20:41:32,112 - __main__ - INFO - Processed 3448/5000 reads\n",
      "2025-08-25 20:41:33,471 - __main__ - INFO - Processed 3528/5000 reads\n",
      "2025-08-25 20:41:34,825 - __main__ - INFO - Processed 3608/5000 reads\n",
      "2025-08-25 20:41:36,177 - __main__ - INFO - Processed 3688/5000 reads\n",
      "2025-08-25 20:41:37,534 - __main__ - INFO - Processed 3768/5000 reads\n",
      "2025-08-25 20:41:38,882 - __main__ - INFO - Processed 3848/5000 reads\n",
      "2025-08-25 20:41:40,237 - __main__ - INFO - Processed 3928/5000 reads\n",
      "2025-08-25 20:41:41,594 - __main__ - INFO - Processed 4008/5000 reads\n",
      "2025-08-25 20:41:42,952 - __main__ - INFO - Processed 4088/5000 reads\n",
      "2025-08-25 20:41:44,312 - __main__ - INFO - Processed 4168/5000 reads\n",
      "2025-08-25 20:41:45,667 - __main__ - INFO - Processed 4248/5000 reads\n",
      "2025-08-25 20:41:47,026 - __main__ - INFO - Processed 4328/5000 reads\n",
      "2025-08-25 20:41:48,380 - __main__ - INFO - Processed 4408/5000 reads\n",
      "2025-08-25 20:41:49,737 - __main__ - INFO - Processed 4488/5000 reads\n",
      "2025-08-25 20:41:51,104 - __main__ - INFO - Processed 4568/5000 reads\n",
      "2025-08-25 20:41:52,463 - __main__ - INFO - Processed 4648/5000 reads\n",
      "2025-08-25 20:41:53,823 - __main__ - INFO - Processed 4728/5000 reads\n",
      "2025-08-25 20:41:55,180 - __main__ - INFO - Processed 4808/5000 reads\n",
      "2025-08-25 20:41:56,536 - __main__ - INFO - Processed 4888/5000 reads\n",
      "2025-08-25 20:41:57,899 - __main__ - INFO - Processed 4968/5000 reads\n",
      "2025-08-25 20:41:58,443 - __main__ - INFO - Quantification completed!\n",
      "2025-08-25 20:41:58,443 - __main__ - INFO - Assignment statistics: {'total_reads': 5000, 'high_confidence': 0, 'unique_match': 22, 'best_match': 4921, 'gene_only': 56, 'unassigned': 1}\n",
      "2025-08-25 20:41:58,540 - __main__ - INFO - Results saved to deepquant_v2_results\n",
      "2025-08-25 20:41:58,540 - __main__ - INFO -   Assignments: deepquant_v2_results/deepquant_v2_assignments.csv\n",
      "2025-08-25 20:41:58,541 - __main__ - INFO -   Counts: deepquant_v2_results/deepquant_v2_transcript_counts.csv\n",
      "2025-08-25 20:41:58,541 - __main__ - INFO -   Stats: deepquant_v2_results/deepquant_v2_stats.json\n",
      "2025-08-25 20:41:58,542 - __main__ - INFO - Evaluating against ground truth...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving results...\n",
      "📈 Evaluating results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 20:41:58,691 - __main__ - INFO - Evaluation report saved to deepquant_v2_results/evaluation_report.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🎯 DEEPQUANT V2 RESULTS SUMMARY\n",
      "============================================================\n",
      "📋 Total reads processed: 5000\n",
      "✅ Assignment rate: 98.9%\n",
      "🎯 Transcript accuracy: 1.3%\n",
      "🧬 Gene accuracy: 0.0%\n",
      "🎲 Average confidence: 0.259\n",
      "📁 Results saved to: ./deepquant_v2_results\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function for DeepQuant V2.\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    config = DeepQuantConfig(\n",
    "        # Model parameters\n",
    "        embedding_dim=256,\n",
    "        gene_embedding_dim=128,\n",
    "        batch_size=8,  # Reduced for memory efficiency\n",
    "        learning_rate=1e-5,\n",
    "        num_epochs=5,\n",
    "        \n",
    "        # Assignment parameters\n",
    "        gene_similarity_threshold=0.7,\n",
    "        isoform_similarity_threshold=0.85,\n",
    "        uncertainty_threshold=0.8,\n",
    "        \n",
    "        # Paths\n",
    "        output_dir=\"./deepquant_v2_results\"\n",
    "    )\n",
    "    \n",
    "    # Initialize DeepQuant V2\n",
    "    dq = DeepQuantV2(config)\n",
    "    \n",
    "    # Load data\n",
    "    transcriptome_path = \"/run/media/saadat/A/tools/DeepQuant/gencode.v47.transcripts_500.fa\"\n",
    "    reads_path = \"/run/media/saadat/A/tools/DeepQuant/simulated_reads.fastq\"\n",
    "    ground_truth_path = \"/run/media/saadat/A/tools/DeepQuant/simulated_ground_truth.csv\"\n",
    "    \n",
    "    print(\"🧬 Loading transcriptome and data...\")\n",
    "    dq.load_data(transcriptome_path, reads_path, ground_truth_path)\n",
    "    \n",
    "    # Initialize and train model\n",
    "    print(\"🤖 Initializing and training model...\")\n",
    "    dq.initialize_model()\n",
    "    dq.train()\n",
    "    \n",
    "    # Build search index\n",
    "    print(\"🔍 Building gene-aware search index...\")\n",
    "    dq.build_index()\n",
    "    \n",
    "    # Quantify reads\n",
    "    print(\"📊 Quantifying reads...\")\n",
    "    results = dq.quantify_reads()\n",
    "    \n",
    "    # Save results\n",
    "    print(\"💾 Saving results...\")\n",
    "    output_files = dq.save_results(results)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"📈 Evaluating results...\")\n",
    "    evaluator = DeepQuantEvaluator(config)\n",
    "    eval_metrics = evaluator.evaluate_against_ground_truth(\n",
    "        results['assignments'], \n",
    "        dq.ground_truth\n",
    "    )\n",
    "    \n",
    "    # Generate evaluation report\n",
    "    evaluator.generate_evaluation_report(eval_metrics, config.output_dir)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 DEEPQUANT V2 RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"📋 Total reads processed: {eval_metrics['total_reads']}\")\n",
    "    print(f\"✅ Assignment rate: {eval_metrics['assignment_rate']:.1%}\")\n",
    "    print(f\"🎯 Transcript accuracy: {eval_metrics['transcript_accuracy']:.1%}\")\n",
    "    print(f\"🧬 Gene accuracy: {eval_metrics['gene_accuracy']:.1%}\")\n",
    "    print(f\"🎲 Average confidence: {eval_metrics['average_confidence']:.3f}\")\n",
    "    print(f\"📁 Results saved to: {config.output_dir}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return dq, results, eval_metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabpfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
